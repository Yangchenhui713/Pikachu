# %% [markdown]
# # **ä½¿ç”¨ LangGraph å’Œ Qwen æ¨¡å‹å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹å››å¤§ç­–ç•¥**
# 
# ### **ç›®æ ‡**
# æœ¬ Notebook å°†ä½œä¸ºä¸€ä»½è¯¦ç»†çš„æŠ€æœ¯æŒ‡å—ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LangGraph æ¡†æ¶å’Œé€šä¹‰åƒé—®ï¼ˆQwenï¼‰æ¨¡å‹ï¼Œä¸€æ­¥æ­¥å®ç°â€œä¸Šä¸‹æ–‡å·¥ç¨‹â€çš„å››å¤§æ ¸å¿ƒç­–ç•¥ã€‚
# 
# å››å¤§ç­–ç•¥åŒ…æ‹¬ï¼š
# 1.  **å†™å…¥ (Write):** ä¸ºæ™ºèƒ½ä½“æ„å»ºä¸€ä¸ªâ€œå¤–éƒ¨å¤§è„‘â€ï¼ˆæš‚å­˜åŒºï¼‰ï¼Œä»¥åœ¨é•¿ä»»åŠ¡ä¸­ä¿æŒçŠ¶æ€ã€‚
# 2.  **é€‰æ‹© (Select):** ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»çŸ¥è¯†åº“ä¸­ç²¾å‡†è°ƒå–ä¿¡æ¯ã€‚
# 3.  **å‹ç¼© (Compress):** æ™ºèƒ½åœ°æ€»ç»“å¯¹è¯å†å²ï¼Œä»¥èŠ‚çœæˆæœ¬å’ŒTokenã€‚
# 4.  **éš”ç¦» (Isolate):** ä½¿ç”¨å¤šæ™ºèƒ½ä½“ï¼ˆMulti-agentï¼‰æ¶æ„ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ç»™ä¸“å®¶å¤„ç†ã€‚
# 
# ---
# ### **ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒè®¾ç½®ä¸æ¨¡å‹åˆå§‹åŒ–**

# %%
# ! pip install langchain langchain-qwq langgraph pandas python-dotenv langchain_community dashscope faiss-cpu pandas

# %%
import os
import getpass
import json
import operator
from typing import List, TypedDict, Annotated

import tiktoken
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool
from langchain_qwq import ChatQwen
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# %%
# --- 1. è®¾ç½®APIå¯†é’¥ ---
# è¯·æ³¨æ„ï¼šè¿™é‡Œéœ€è¦çš„æ˜¯ DashScope çš„ API Key
if "DASHSCOPE_API_KEY" not in os.environ:
    os.environ["DASHSCOPE_API_KEY"] = getpass.getpass("è¯·è¾“å…¥æ‚¨çš„DashScope API Key: ")

# %%
# --- 2. åˆå§‹åŒ–Qwenæ¨¡å‹ ---
# æˆ‘ä»¬å°†ä½¿ç”¨ qwen3-32b æ¨¡å‹ä½œä¸ºæˆ‘ä»¬æ™ºèƒ½ä½“çš„â€œå¤§è„‘â€
try:
    model = ChatQwen(model="qwen3-30b-a3b", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  enable_thinking=False)
    print("Qwen æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
except Exception as e:
    print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyæˆ–ç½‘ç»œè¿æ¥: {e}")

# %%
# --- 3. åˆå§‹åŒ–Tokenè®¡ç®—å™¨ ---
# è¿™å°†å¸®åŠ©æˆ‘ä»¬é‡åŒ–â€œå‹ç¼©â€ç­–ç•¥å¸¦æ¥çš„æ•ˆæœ
encoding = tiktoken.get_encoding("cl100k_base")

# %% [markdown]
# ## **ç­–ç•¥ä¸€ï¼šå†™å…¥ (Write) - æ„å»ºæ™ºèƒ½ä½“çš„â€œè‰ç¨¿çº¸â€**
# 
# **æ ¸å¿ƒæ€æƒ³:** ä¸æŠŠæ‰€æœ‰ä¸­é—´æ­¥éª¤å’Œæ€è€ƒéƒ½å¡è¿›ä¸»å¯¹è¯å†å²ï¼ˆ`messages`ï¼‰ï¼Œè€Œæ˜¯å°†å®ƒä»¬â€œå†™å…¥â€åˆ°ä¸€ä¸ªç‹¬ç«‹çš„â€œæš‚å­˜åŒºâ€ï¼ˆ`scratchpad`ï¼‰ã€‚è¿™å¯ä»¥ä¿æŒä¸»å¯¹è¯çš„æ¸…æ™°ï¼Œå¹¶ä¸ºæ™ºèƒ½ä½“æä¾›ä¸€ä¸ªå¯é çš„çŸ­æœŸè®°å¿†ï¼Œé˜²æ­¢åœ¨é•¿ä»»åŠ¡ä¸­â€œå¤±å¿†â€ã€‚
# 
# **å®ç°:** æˆ‘ä»¬å°†åœ¨`AgentState`ä¸­å¢åŠ ä¸€ä¸ª`scratchpad`å­—æ®µï¼Œå¹¶é€šè¿‡æ·»åŠ `SystemMessage`æ¥æŒ‡å¯¼æ¨¡å‹çš„è¡Œä¸ºï¼Œé˜²æ­¢æ— é™å¾ªç¯ã€‚

# %%
# --- 1. å®šä¹‰çŠ¶æ€ ---
from typing_extensions import TypedDict
from typing import List, Annotated
import operator

class ToolCallRecord(TypedDict):
    step: int
    tool_name: str
    args: dict
    result: str

class WriteStrategyState(TypedDict):
    messages: Annotated[list, operator.add]
    # ç»“æ„åŒ– scratchpadï¼Œä¿ç•™å®Œæ•´å†å²
    scratchpad: dict  # {"history": List[ToolCallRecord], "final_answer": str}

# %%
# --- 2. å®šä¹‰å·¥å…· ---
from langchain_core.tools import tool

@tool
def simple_calculator(operation: str, a: int, b: int) -> int:
    """ä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨å·¥å…·ï¼Œæ‰§è¡ŒåŠ å‡ä¹˜é™¤ã€‚"""
    if operation == "add":
        return a + b
    if operation == "subtract":
        return a - b
    if operation == "multiply":
        return a * b
    if operation == "divide" and b != 0:
        return a // b
    return "æ— æ•ˆæ“ä½œ"

# %%
# --- 3. å®šä¹‰å›¾çš„èŠ‚ç‚¹ ---
from langgraph.prebuilt import ToolNode
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import END

tools = [simple_calculator]
tool_node = ToolNode(tools)
model_with_tools = model.bind_tools(tools)

def agent_with_scratchpad(state: WriteStrategyState):
    """
    Agent èŠ‚ç‚¹ï¼šå†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œå¹¶æ›´æ–°æš‚å­˜åŒºã€‚
    """
    print("---AGENT NODE---")
    response = model_with_tools.invoke(state['messages'])

    if response.tool_calls:
        # æš‚å­˜å½“å‰å¾…æ‰§è¡Œçš„å·¥å…·
        state['scratchpad']['pending_tool'] = response.tool_calls[0]
        print(f"ğŸ§  Agent Action: Call tool `{response.tool_calls[0]['name']}` "
              f"with arguments `{response.tool_calls[0]['args']}`")
    else:
        # å…¨éƒ¨å®Œæˆï¼Œä¿å­˜æœ€ç»ˆç­”æ¡ˆ
        state['scratchpad']['final_answer'] = response.content
        print(f"âœ… Final Answer: {response.content}")

    return {"messages": [response], "scratchpad": state['scratchpad']}

def tool_node_with_scratchpad(state: WriteStrategyState):
    """
    Tool èŠ‚ç‚¹ï¼šæ‰§è¡Œå·¥å…·ï¼Œå¹¶æŠŠç»“æœè®°å½•åˆ° historyã€‚
    """
    print("---TOOL NODE---")
    last_message = state['messages'][-1]
    tool_messages = tool_node.invoke([last_message])

    # å–å‡ºå¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
    pending = state['scratchpad']['pending_tool']
    record = ToolCallRecord(
        step=len(state['scratchpad'].get("history", [])) + 1,
        tool_name=pending['name'],
        args=pending['args'],
        result=str(tool_messages[0].content)
    )
    # è¿½åŠ åˆ°å†å²
    state['scratchpad'].setdefault("history", []).append(record)
    print(f"ğŸ“ Recorded Tool Call: {record}")
    state['scratchpad'].pop("pending_tool", None)  # æ¸…ç†

    #print(f"ğŸ› ï¸ Tool Result: `{record['result']}`")
    return {"messages": tool_messages, "scratchpad": state['scratchpad']}

# %%
# --- 4. æ„å»ºå›¾ ---
from langgraph.graph import StateGraph

write_graph_builder = StateGraph(WriteStrategyState)
write_graph_builder.add_node("agent", agent_with_scratchpad)
write_graph_builder.add_node("action", tool_node_with_scratchpad)
write_graph_builder.set_entry_point("agent")

# æ¡ä»¶è¾¹ï¼šæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„å·¥å…·
def should_continue(state: WriteStrategyState) -> str:
    # è‹¥æœ€ç»ˆç­”æ¡ˆå·²å­˜åœ¨ï¼Œç›´æ¥ç»“æŸ
    if state['scratchpad'].get("final_answer"):
        return END
    return "action"

write_graph_builder.add_conditional_edges("agent", should_continue, {"action": "action", END: END})
write_graph_builder.add_edge("action", "agent")
write_graph = write_graph_builder.compile()

# %%
# --- 5. æ¼”ç¤º ---
print("### æ¼”ç¤ºâ€œå†™å…¥â€ç­–ç•¥ ###")
task = (
    "1) åˆå§‹ç°é‡‘æµ 128 å…ƒä¸é¢„ç®—è¿½åŠ  72 å…ƒå…ˆè¿›è¡Œåˆå¹¶ï¼›\n"
    "2) åˆå¹¶åçš„èµ„é‡‘æŒ‰å­£åº¦å¤åˆ© 3 å€æ æ†æ”¾å¤§ï¼›\n"
    "3) æ”¾å¤§åçš„èµ„é‡‘å› æ±‡ç‡æŠ˜ç®—éœ€é™¤ä»¥ 100 å¾—åˆ°åŸºå‡†å•ä½å€¼ï¼›\n"
    "4) åŸºå‡†å•ä½å€¼å†æŒ‰ 20 å€é£é™©ç³»æ•°æ”¾å¤§ï¼Œå½¢æˆé£é™©æ•å£ï¼›\n"
    "5) æœ€ç»ˆä»é£é™©æ•å£ä¸­ä¸€æ¬¡æ€§æ‰£é™¤ 222 å…ƒçš„å›ºå®šå‡†å¤‡é‡‘ã€‚\n"
    "è¯·åˆ—å‡ºæ¯ä¸€æ­¥çš„æ•°å€¼ç»“æœï¼Œå¹¶ä»¥ã€æœ€ç»ˆç»“æœï¼š{æ•°å€¼}ã€çš„æ ¼å¼ç»™å‡ºç­”æ¡ˆã€‚"
)

system_prompt = (
    "ä½ æ˜¯ä¸€ä¸ªè®¡ç®—åŠ©æ‰‹ã€‚è¯·æŒ‰æ­¥éª¤ä½¿ç”¨ `simple_calculator` å·¥å…·æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ "
)
initial_messages = [
    SystemMessage(content=system_prompt),
    HumanMessage(content=task)
]
initial_state = {"messages": initial_messages, "scratchpad": {"history": [], "final_answer": None}}

# ä½¿ç”¨ .stream() è§‚å¯Ÿæ¯ä¸€æ­¥
for step in write_graph.stream(initial_state, {"recursion_limit": 20}):
    #print(step)
    print("---")

# %% [markdown]
# ## **ç­–ç•¥äºŒï¼šé€‰æ‹© (Select) - ç²¾å‡†çš„"ä¿¡æ¯è°ƒå–"**
# 
# **æ ¸å¿ƒæ€æƒ³ï¼š** ä½¿ç”¨RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ï¼Œä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­ç²¾å‡†æ£€ç´¢æœ€ç›¸å…³çš„ä¿¡æ¯ç‰‡æ®µï¼Œåªå°†å¿…è¦ä¿¡æ¯æ³¨å…¥ä¸Šä¸‹æ–‡ã€‚
# 
# **å®ç°æ­¥éª¤ï¼š**
# 1. åˆ›å»ºäº§å“çŸ¥è¯†åº“ï¼ˆæ¨¡æ‹Ÿå‘é‡æ•°æ®åº“ï¼‰
# 2. æ„å»ºRAGæ£€ç´¢å™¨
# 3. è®¾è®¡æ™ºèƒ½ä½“æµç¨‹ï¼šé—®é¢˜ â†’ æ£€ç´¢ â†’ ç”Ÿæˆç­”æ¡ˆ
# 4. å¯è§†åŒ–TokenèŠ‚çœæ•ˆæœ

# %%
# --- 1. åˆ›å»ºæ¨¡æ‹Ÿäº§å“çŸ¥è¯†åº“ ---
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.embeddings import DashScopeEmbeddings

# åˆ›å»ºåµŒå…¥æ¨¡å‹
embeddings = DashScopeEmbeddings(model="text-embedding-v3")

# %%
# äº§å“çŸ¥è¯†æ–‡æ¡£ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä»æ•°æ®åº“åŠ è½½ï¼‰
product_docs = [
    Document(page_content="""æœºæ¢°é”®ç›˜ X1 Pro æŠ€æœ¯è§„æ ¼ï¼š
- è½´ä½“ï¼šå®šåˆ¶é’è½´ï¼Œ60gè§¦å‘å‹åŠ›
- è¿æ¥ï¼šä¸‰æ¨¡ï¼ˆè“ç‰™5.1/2.4G/USB-Cï¼‰
- ç”µæ± ï¼š4000mAhï¼Œç»­èˆª200å°æ—¶
- ç‰¹ç‚¹ï¼šçƒ­æ’æ‹”è½´ä½“ï¼ŒPBTåŒè‰²é”®å¸½ï¼Œå…¨é”®æ— å†²
- ä»·æ ¼ï¼š699å…ƒï¼ˆé™æ—¶ä¼˜æƒ 599å…ƒï¼‰""", 
             metadata={"product": "æœºæ¢°é”®ç›˜ X1 Pro", "category": "é”®ç›˜"}),
    
    Document(page_content="""æ¸¸æˆé¼ æ ‡ M800 æ——èˆ°ç‰ˆï¼š
- ä¼ æ„Ÿå™¨ï¼šåŸç›¸PAW3395ï¼Œ26000DPI
- å¾®åŠ¨ï¼šæ¬§å§†é¾™å…‰å­¦å¾®åŠ¨ï¼Œ1äº¿æ¬¡å¯¿å‘½
- é‡é‡ï¼š58gï¼ˆè¶…è½»é‡åŒ–è®¾è®¡ï¼‰
- RGBï¼š1680ä¸‡è‰²ï¼Œ10åŒºåŸŸç‹¬ç«‹æ§å…‰
- ä»·æ ¼ï¼š399å…ƒï¼ˆå¥—è£…ä¼˜æƒ ä»·ï¼‰""", 
             metadata={"product": "æ¸¸æˆé¼ æ ‡ M800", "category": "é¼ æ ‡"}),
    
    Document(page_content="""ä¿ƒé”€é‚®ä»¶å†™ä½œæŒ‡å—ï¼š
1. æ ‡é¢˜è¦å¸å¼•çœ¼çƒï¼ŒåŒ…å«ä¼˜æƒ ä¿¡æ¯
2. å¼€å¤´ç”¨ç—›ç‚¹åœºæ™¯å¼•å‘å…±é¸£
3. çªå‡ºäº§å“æ ¸å¿ƒä¼˜åŠ¿ï¼ˆæ€§èƒ½>å‚æ•°ï¼‰
4. é™æ—¶ä¼˜æƒ åˆ¶é€ ç´§è¿«æ„Ÿ
5. æ¸…æ™°çš„è¡ŒåŠ¨å¬å”¤æŒ‰é’®""", 
             metadata={"doc_type": "writing_guide"}),
    
    Document(page_content="""ç”¨æˆ·åå¥½åˆ†æï¼š
ç§‘æŠ€äº§å“æ¶ˆè´¹è€…æœ€å…³æ³¨ï¼š
- æ€§èƒ½å‚æ•°ï¼ˆ75%ç”¨æˆ·ï¼‰
- æ€§ä»·æ¯”ï¼ˆ68%ç”¨æˆ·ï¼‰
- è€ç”¨æ€§ï¼ˆ52%ç”¨æˆ·ï¼‰
- å¤–è§‚è®¾è®¡ï¼ˆ48%ç”¨æˆ·ï¼‰""", 
             metadata={"doc_type": "user_insight"}),
]

# åˆ›å»ºå‘é‡æ•°æ®åº“
vector_db = FAISS.from_documents(product_docs, embeddings)

# %%
# --- 2. æ„å»ºRAGæ£€ç´¢å™¨ ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vector_db.as_retriever(search_kwargs={"k": 2})

def format_docs(docs):
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    return "\n\n".join(f"## æ¥æº {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs))

# åˆ›å»ºRAGæç¤ºæ¨¡æ¿
rag_template = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§å“æ–‡æ¡ˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„èƒŒæ™¯ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

<èƒŒæ™¯ä¿¡æ¯>
{context}
</èƒŒæ™¯ä¿¡æ¯>

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€å›ç­”ï¼Œçªå‡ºäº§å“æ ¸å¿ƒä¼˜åŠ¿ï¼š
"""
rag_prompt = ChatPromptTemplate.from_template(rag_template)

# åˆ›å»ºRAGé“¾
rag_chain = (
    {"context": retriever | RunnableLambda(format_docs), "question": RunnablePassthrough()}
    | rag_prompt
    | model
    | StrOutputParser()
)


# %%
# --- 3. è®¾è®¡æ™ºèƒ½ä½“æµç¨‹ ---
class SelectStrategyState(TypedDict):
    messages: List[BaseMessage]
    context: str  # å­˜å‚¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡

def retrieve_context(state: SelectStrategyState):
    """æ£€ç´¢èŠ‚ç‚¹ï¼šä»çŸ¥è¯†åº“è·å–ç›¸å…³ä¿¡æ¯"""
    print("\n--- RETRIEVE CONTEXT ---")
    last_message = state["messages"][-1].content
    
    # æ‰§è¡Œæ£€ç´¢
    docs = retriever.invoke(last_message)
    context = format_docs(docs)
    
    # è®¡ç®—TokenèŠ‚çœ
    orig_token_count = sum(len(encoding.encode(doc.page_content)) for doc in docs)
    context_token_count = len(encoding.encode(context))
    savings = orig_token_count - context_token_count
    
    print(f"ğŸ” æ£€ç´¢åˆ° {len(docs)} æ¡ç›¸å…³æ–‡æ¡£")
    print(f"ğŸ“‰ TokenèŠ‚çœ: {savings} (åŸå§‹: {orig_token_count} -> å‹ç¼©: {context_token_count})")
    print(f"ğŸ“ æ³¨å…¥ä¸Šä¸‹æ–‡:\n{context[:300]}...")
    
    return {"context": context}

def generate_with_context(state: SelectStrategyState):
    """ç”ŸæˆèŠ‚ç‚¹ï¼šä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
    print("\n--- GENERATE WITH CONTEXT ---")
    question = state["messages"][-1].content
    
    # ä½¿ç”¨RAGé“¾ç”Ÿæˆå›ç­”
    response = rag_chain.invoke(question)
    
    # åˆ›å»ºæ¶ˆæ¯å¯¹è±¡
    response_message = HumanMessage(content=response)
    
    # è¾“å‡ºç»“æœ
    print(f"ğŸ’¡ ç”Ÿæˆçš„å›ç­”: {response}")
    return {"messages": [response_message]}


# %%
# --- 4. æ„å»ºé€‰æ‹©ç­–ç•¥å›¾ ---
from langgraph.graph import StateGraph

# å®šä¹‰çŠ¶æ€
class SelectState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    context: str

# åˆ›å»ºå›¾
select_graph = StateGraph(SelectState)

# æ·»åŠ èŠ‚ç‚¹
select_graph.add_node("retrieve", retrieve_context)
select_graph.add_node("generate", generate_with_context)

# è®¾ç½®å…¥å£ç‚¹
select_graph.set_entry_point("retrieve")

# æ·»åŠ è¾¹
select_graph.add_edge("retrieve", "generate")
select_graph.add_edge("generate", END)

# ç¼–è¯‘å›¾
select_workflow = select_graph.compile()

# %%
# --- 5. æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ ---
print("\n### æ¼”ç¤º'é€‰æ‹©'ç­–ç•¥ ###")
question = "è¯·ä¸ºæˆ‘ä»¬çš„æ——èˆ°æœºæ¢°é”®ç›˜X1 Proå†™ä¸€å°ä¿ƒé”€é‚®ä»¶ï¼Œçªå‡ºå…¶æ ¸å¿ƒä¼˜åŠ¿"

# åˆå§‹çŠ¶æ€
initial_state = SelectState(
    messages=[HumanMessage(content=question)],
    context=""
)

# æ‰§è¡Œå·¥ä½œæµ
for step in select_workflow.stream(initial_state):
    if "__end__" not in step:
        print(step)
        print("---")

# %% [markdown]
# ## **ç­–ç•¥ä¸‰ï¼šå‹ç¼© (Compress) - ä¸ºä¸Šä¸‹æ–‡"ç˜¦èº«å‡è´Ÿ"**
# 
# **æ ¸å¿ƒæ€æƒ³ï¼š** ä½¿ç”¨æ€»ç»“(summarization)å’Œè£å‰ª(trimming)æŠ€æœ¯å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒèŠ‚çœTokenå¹¶æé«˜æ•ˆç‡ã€‚
# 
# **å®ç°ä¸¤ç§å‹ç¼©æŠ€æœ¯ï¼š**
# 1. **æ€»ç»“å‹ç¼©**ï¼šå°†é•¿æ–‡æœ¬æç‚¼ä¸ºç®€æ´æ‘˜è¦
# 2. **è£å‰ªå‹ç¼©**ï¼šæ™ºèƒ½ä¿ç•™å¯¹è¯ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†
# 
# **åœºæ™¯æ¼”ç¤ºï¼š** æ™ºèƒ½ä½“éœ€è¦é˜…è¯»ä¸€ç¯‡é•¿æ–‡ç« å¹¶å›ç­”é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡æ€»ç»“å‹ç¼©æ–‡ç« å†…å®¹ï¼›åŒæ—¶å±•ç¤ºå¯¹è¯å†å²è£å‰ªæŠ€æœ¯ã€‚

# %%
# --- 1. å‡†å¤‡é•¿æ–‡æœ¬ç¤ºä¾‹ ---
long_article = """
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ­£ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦æ¨è¿›ã€‚2023å¹´ï¼ŒOpenAIå‘å¸ƒäº†GPT-4æ¨¡å‹ï¼Œå…¶ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ°32K tokensï¼Œå¤§å¤§å¢å¼ºäº†å¤„ç†é•¿æ–‡æ¡£çš„èƒ½åŠ›ã€‚éšåï¼ŒAnthropicæ¨å‡ºäº†Claude 2.1æ¨¡å‹ï¼Œæ”¯æŒ200K tokensçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œåˆ›ä¸‹äº†å½“æ—¶çš„æ–°çºªå½•ã€‚

ç„¶è€Œï¼Œ2024å¹´ï¼Œè¿™ä¸€çºªå½•è¢«ä¸­å›½ç§‘æŠ€å…¬å¸æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰æ‰“ç ´ã€‚ä»–ä»¬å‘å¸ƒäº†DeepSeek-R1æ¨¡å‹ï¼Œä¸ä»…æ”¯æŒ128K tokensçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿˜åˆ›æ–°æ€§åœ°å¼•å…¥äº†"ä¸Šä¸‹æ–‡å‹ç¼©"æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ™ºèƒ½æ€»ç»“å’Œå…³é”®ä¿¡æ¯æå–ï¼Œå¯ä»¥å°†é•¿æ–‡æ¡£å‹ç¼©åˆ°åŸé•¿åº¦çš„20%-30%ï¼ŒåŒæ—¶ä¿ç•™95%ä»¥ä¸Šçš„æ ¸å¿ƒä¿¡æ¯ã€‚

DeepSeek-R1çš„æŠ€æœ¯åˆ›æ–°ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼š
1. åˆ†å±‚æ€»ç»“æ¶æ„ï¼šæ¨¡å‹é¦–å…ˆå¯¹æ–‡æ¡£è¿›è¡Œåˆ†æ®µæ€»ç»“ï¼Œç„¶åå¯¹åˆ†æ®µæ‘˜è¦è¿›è¡ŒäºŒæ¬¡æ€»ç»“ï¼Œå½¢æˆå±‚æ¬¡åŒ–çš„å‹ç¼©ç»“æ„ã€‚
2. è¯­ä¹‰å¯†åº¦ä¼˜åŒ–ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ¨¡å‹å­¦ä¼šè¯†åˆ«å¹¶ä¿ç•™ä¿¡æ¯å¯†åº¦æœ€é«˜çš„å†…å®¹ã€‚
3. è‡ªé€‚åº”å‹ç¼©ç‡ï¼šæ ¹æ®ç”¨æˆ·ä»»åŠ¡ç±»å‹åŠ¨æ€è°ƒæ•´å‹ç¼©å¼ºåº¦ï¼Œå¹³è¡¡ä¿¡æ¯ä¿ç•™ä¸æ•ˆç‡ã€‚

åœ¨å®é™…æµ‹è¯•ä¸­ï¼ŒDeepSeek-R1å¤„ç†ä¸€ç¯‡10,000å­—çš„ç§‘æŠ€è®ºæ–‡æ—¶ï¼Œå°†å…¶å‹ç¼©åˆ°1,500å­—çš„å…³é”®æ‘˜è¦ï¼ŒåŒæ—¶å‡†ç¡®å›ç­”äº†è®ºæ–‡ä¸­çš„æ ¸å¿ƒé—®é¢˜ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œå‹ç¼©åçš„Tokenä½¿ç”¨é‡ä»…ä¸ºåŸå§‹çš„18%ï¼Œè€Œä»»åŠ¡å®Œæˆè´¨é‡ä»…ä¸‹é™2%ã€‚

è¿™é¡¹æŠ€æœ¯çš„å•†ä¸šåº”ç”¨å‰æ™¯å¹¿é˜”ï¼š
- æ³•å¾‹è¡Œä¸šï¼šå¿«é€Ÿåˆ†æå†—é•¿çš„æ³•å¾‹æ–‡ä»¶
- é‡‘èé¢†åŸŸï¼šé«˜æ•ˆå¤„ç†å¹´åº¦è´¢æŠ¥å’Œæ‹›è‚¡ä¹¦
- å­¦æœ¯ç ”ç©¶ï¼šåŠ é€Ÿæ–‡çŒ®ç»¼è¿°è¿‡ç¨‹
- å®¢æˆ·æœåŠ¡ï¼šå¿«é€Ÿç†è§£é•¿ç¯‡å®¢æˆ·åé¦ˆ

DeepSeekå›¢é˜Ÿè¡¨ç¤ºï¼Œä»–ä»¬ä¸‹ä¸€æ­¥å°†æ¢ç´¢"åŠ¨æ€ä¸Šä¸‹æ–‡å‹ç¼©"ï¼Œå³åœ¨å¯¹è¯è¿‡ç¨‹ä¸­å®æ—¶è°ƒæ•´å‹ç¼©ç‡ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„é•¿æœŸè®°å¿†ç®¡ç†ã€‚
"""

# %%
# --- 2. å®šä¹‰å‹ç¼©å·¥å…· ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# æ€»ç»“å‹ç¼©å·¥å…·
summary_prompt = ChatPromptTemplate.from_template(
    "è¯·å°†ä»¥ä¸‹æ–‡æœ¬æ€»ç»“ä¸ºä¸è¶…è¿‡{max_words}å­—çš„å…³é”®è¦ç‚¹ï¼Œä¿ç•™æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯å’Œæ•°æ®ï¼š\n\n{text}"
)

summarizer_chain = (
    summary_prompt
    | model
    | StrOutputParser()
)

# è£å‰ªå‹ç¼©å‡½æ•°
def trim_messages(messages: List[BaseMessage], max_messages=5) -> List[BaseMessage]:
    """è£å‰ªå¯¹è¯å†å²ï¼Œä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€æ–°çš„å‡ æ¡æ¶ˆæ¯"""
    # å§‹ç»ˆä¿ç•™ç¬¬ä¸€æ¡ç³»ç»Ÿæ¶ˆæ¯
    system_message = messages[0] if messages and isinstance(messages[0], SystemMessage) else None
    
    # ä¿ç•™æœ€è¿‘çš„max_messagesæ¡æ¶ˆæ¯ï¼ˆæ’é™¤ç³»ç»Ÿæ¶ˆæ¯ï¼‰
    recent_messages = messages[-max_messages:] if len(messages) > 1 else messages
    
    # é‡æ–°ç»„åˆ
    trimmed = []
    if system_message:
        trimmed.append(system_message)
    trimmed.extend(recent_messages)
    
    return trimmed

# %%
# --- 3. å®šä¹‰çŠ¶æ€å’ŒèŠ‚ç‚¹ ---
class CompressState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    original_text: str  # åŸå§‹é•¿æ–‡æœ¬
    compressed_text: str  # å‹ç¼©åçš„æ–‡æœ¬
    token_savings: int  # èŠ‚çœçš„Tokenæ•°é‡

def compress_long_text(state: CompressState):
    """æ€»ç»“å‹ç¼©èŠ‚ç‚¹ï¼šå°†é•¿æ–‡æœ¬å‹ç¼©ä¸ºæ‘˜è¦"""
    print("\n--- COMPRESSING LONG TEXT ---")
    last_message = state["messages"][-1]
    
    # ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–é—®é¢˜
    question = last_message.content
    
    # å‹ç¼©é•¿æ–‡æœ¬
    summary = summarizer_chain.invoke({"text": state["original_text"], "max_words": 300})
    
    # è®¡ç®—TokenèŠ‚çœ
    orig_tokens = len(encoding.encode(state["original_text"]))
    comp_tokens = len(encoding.encode(summary))
    savings = orig_tokens - comp_tokens
    
    print(f"ğŸ“‰ æ–‡æœ¬å‹ç¼©: {orig_tokens} tokens â†’ {comp_tokens} tokens (èŠ‚çœ {savings} tokens)")
    print(f"ğŸ“ å‹ç¼©æ‘˜è¦:\n{summary[:200]}...")
    
    # æ›´æ–°çŠ¶æ€
    return {
        "compressed_text": summary,
        "token_savings": savings,
        "messages": [HumanMessage(content=f"åŸºäºä»¥ä¸‹æ‘˜è¦å›ç­”é—®é¢˜:\n{summary}\n\né—®é¢˜: {question}")]
    }

def answer_with_compressed_text(state: CompressState):
    """å›ç­”èŠ‚ç‚¹ï¼šåŸºäºå‹ç¼©æ–‡æœ¬å›ç­”é—®é¢˜"""
    print("\n--- ANSWERING WITH COMPRESSED TEXT ---")
    
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    response = model.invoke(state["messages"])
    answer = response.content
    
    print(f"ğŸ’¡ ç”Ÿæˆçš„å›ç­”: {answer[:200]}...")
    return {"messages": [response]}

def trim_context(state: CompressState):
    """è£å‰ªèŠ‚ç‚¹ï¼šå‹ç¼©å¯¹è¯å†å²"""
    print("\n--- TRIMMING CONTEXT ---")
    
    # è®¡ç®—è£å‰ªå‰çš„Token
    all_messages = "".join(m.content for m in state["messages"])
    before_tokens = len(encoding.encode(all_messages))
    
    # æ‰§è¡Œè£å‰ª
    trimmed_messages = trim_messages(state["messages"], max_messages=3)
    
    # è®¡ç®—è£å‰ªåçš„Token
    trimmed_content = "".join(m.content for m in trimmed_messages)
    after_tokens = len(encoding.encode(trimmed_content))
    savings = before_tokens - after_tokens
    
    print(f"âœ‚ï¸ è£å‰ªå†å²: {len(state['messages'])}æ¡ â†’ {len(trimmed_messages)}æ¡æ¶ˆæ¯")
    print(f"ğŸ“‰ TokenèŠ‚çœ: {savings} (åŸå§‹: {before_tokens} -> è£å‰ªå: {after_tokens})")
    
    return {"messages": trimmed_messages}

# %%
# --- 4. æ„å»ºå‹ç¼©ç­–ç•¥å›¾ ---
compress_graph = StateGraph(CompressState)

# æ·»åŠ èŠ‚ç‚¹
compress_graph.add_node("compress", compress_long_text)
compress_graph.add_node("answer", answer_with_compressed_text)
compress_graph.add_node("trim", trim_context)

# è®¾ç½®å…¥å£ç‚¹
compress_graph.set_entry_point("compress")

# æ·»åŠ è¾¹
compress_graph.add_edge("compress", "answer")
compress_graph.add_edge("answer", END)

# æ·»åŠ æ¡ä»¶è¾¹ç”¨äºè£å‰ª
def should_trim(state: CompressState):
    """å½“æ¶ˆæ¯è¶…è¿‡5æ¡æ—¶è§¦å‘è£å‰ª"""
    if len(state["messages"]) > 5:
        return "trim"
    return END

compress_graph.add_conditional_edges("answer", should_trim, {"trim": "trim", END: END})
compress_graph.add_edge("trim", END)

# ç¼–è¯‘å›¾
compress_workflow = compress_graph.compile()

# %%
# --- 5. æ¼”ç¤ºæ€»ç»“å‹ç¼© ---
print("\n### æ¼”ç¤º'æ€»ç»“å‹ç¼©'æŠ€æœ¯ ###")
question = "DeepSeek-R1åœ¨æ–‡æœ¬å‹ç¼©æ–¹é¢æœ‰å“ªäº›æŠ€æœ¯åˆ›æ–°ï¼Ÿå‹ç¼©æ•ˆæœå¦‚ä½•ï¼Ÿ"

# åˆå§‹çŠ¶æ€
initial_state = CompressState(
    messages=[SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªAIæŠ€æœ¯åˆ†æå¸ˆ"), HumanMessage(content=question)],
    original_text=long_article,
    compressed_text="",
    token_savings=0
)

# æ‰§è¡Œå·¥ä½œæµ
for step in compress_workflow.stream(initial_state):
    if "__end__" not in step:
        print(step)
        print("---")


# %%
# --- 6. æ¼”ç¤ºå¯¹è¯å†å²è£å‰ª ---
print("\n### æ¼”ç¤º'è£å‰ªå‹ç¼©'æŠ€æœ¯ ###")

# åˆ›å»ºä¸€ä¸ªé•¿å¯¹è¯å†å²
long_chat_history = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡ŒåŠ©æ‰‹"),
    HumanMessage(content="æˆ‘æƒ³è®¡åˆ’ä¸€æ¬¡å»æ—¥æœ¬çš„æ—…è¡Œ"),
    HumanMessage(content="æ—¶é—´å¤§æ¦‚æ˜¯æ˜å¹´3æœˆä¸‹æ—¬ï¼Œ10å¤©å·¦å³"),
    HumanMessage(content="æˆ‘å¯¹äº¬éƒ½çš„æ–‡åŒ–æ™¯ç‚¹ç‰¹åˆ«æ„Ÿå…´è¶£"),
    HumanMessage(content="å¦å¤–ä¹Ÿæƒ³ä½“éªŒä¸€ä¸‹ä¸œäº¬çš„ç°ä»£åŒ–éƒ½å¸‚"),
    HumanMessage(content="é¢„ç®—æ–¹é¢å¸Œæœ›æ§åˆ¶åœ¨2ä¸‡å…ƒä»¥å†…"),
    HumanMessage(content="è¯·å¸®æˆ‘è§„åˆ’ä¸€ä¸ªè¡Œç¨‹"),
    HumanMessage(content="å¯¹äº†ï¼Œæˆ‘è¿˜æƒ³ä½“éªŒä¸€æ¬¡æ¸©æ³‰æ—…é¦†"),
    HumanMessage(content="æœ€å¥½æ˜¯é‚£ç§ä¼ ç»Ÿçš„æ—¥å¼æ—…é¦†"),
    HumanMessage(content="ç°åœ¨è¯·ç»™æˆ‘å…·ä½“çš„è¡Œç¨‹å»ºè®®")
]

# åˆå§‹çŠ¶æ€ï¼ˆæ— æ–‡æœ¬å‹ç¼©ï¼‰
trim_demo_state = CompressState(
    messages=long_chat_history,
    original_text="",
    compressed_text="",
    token_savings=0
)

# æ‰§è¡Œè£å‰ª
trimmed_state = trim_context(trim_demo_state)

# æ˜¾ç¤ºè£å‰ªæ•ˆæœ
print("\nè£å‰ªå‰æ¶ˆæ¯:")
for i, msg in enumerate(long_chat_history):
    prefix = "ğŸ¤–" if isinstance(msg, SystemMessage) else "ğŸ‘¤"
    print(f"{prefix} {msg.content[:50]}{'...' if len(msg.content) > 50 else ''}")

print("\nè£å‰ªåæ¶ˆæ¯:")
for i, msg in enumerate(trimmed_state['messages']):
    prefix = "ğŸ¤–" if isinstance(msg, SystemMessage) else "ğŸ‘¤"
    print(f"{prefix} {msg.content[:50]}{'...' if len(msg.content) > 50 else ''}")


# %% [markdown]
# ## **ç­–ç•¥å››ï¼šéš”ç¦» (Isolate) - "åˆ†è€Œæ²»ä¹‹"çš„æ¶æ„æ™ºæ…§**
# 
# **æ ¸å¿ƒæ€æƒ³ï¼š** å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œç”±ä¸“é—¨çš„æ™ºèƒ½ä½“åœ¨éš”ç¦»ç¯å¢ƒä¸­å¤„ç†ï¼Œé¿å…ä¸Šä¸‹æ–‡æ±¡æŸ“ã€‚
# 
# **å®ç°å¤šæ™ºèƒ½ä½“æ¶æ„ï¼š** åˆ›å»ºä¸€ä¸ªç”±ä¸»ç®¡ï¼ˆSupervisorï¼‰åè°ƒçš„ä¸“å®¶å›¢é˜Ÿï¼ˆåˆ†æå¸ˆ+æ–‡æ¡ˆï¼‰ã€‚

# %%
# --- 1. å‡†å¤‡æ•°æ® ---
import pandas as pd
from io import StringIO

# åˆ›å»ºç¤ºä¾‹é”€å”®æ•°æ®CSV
sales_data = """
æ—¥æœŸ,äº§å“,é”€å”®é¢,é”€å”®é‡
2024-01-01,æœºæ¢°é”®ç›˜,12800,32
2024-01-01,æ¸¸æˆé¼ æ ‡,9800,49
2024-01-02,æœºæ¢°é”®ç›˜,14500,36
2024-01-02,æ¸¸æˆé¼ æ ‡,10200,51
2024-01-03,æœºæ¢°é”®ç›˜,16200,40
2024-01-03,æ¸¸æˆé¼ æ ‡,10800,54
2024-01-04,æœºæ¢°é”®ç›˜,13800,34
2024-01-04,æ¸¸æˆé¼ æ ‡,11200,56
2024-01-05,æœºæ¢°é”®ç›˜,17500,42
2024-01-05,æ¸¸æˆé¼ æ ‡,11800,59
"""

# ä¿å­˜ä¸ºCSVæ–‡ä»¶
with open("sales_data.csv", "w") as f:
    f.write(sales_data)

# %%
# --- 2. å®šä¹‰çŠ¶æ€ ---
# å®šä¹‰å¤šæ™ºèƒ½ä½“åä½œçš„çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    task: str
    analysis_result: str
    final_output: str
    next_agent: str

# %%
# --- 3. å®šä¹‰å·¥å…· ---
@tool
def analyze_sales_data(question: str) -> str:
    """
    åˆ†æé”€å”®æ•°æ®CSVæ–‡ä»¶ï¼Œæ‰¾å‡ºé”€å”®é¢æœ€é«˜çš„äº§å“åŠå…¶é”€å”®æ€»é¢ã€‚
    å‚æ•°:
        question (str): ç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼Œç”¨äºè®°å½•åˆ†æèƒŒæ™¯ã€‚
    è¿”å›:
        str: ä¸€ä¸ªé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼ŒåŒ…å«äº§å“åç§°å’Œæ€»é”€å”®é¢ï¼Œä¾‹å¦‚ "äº§å“A,150000"ã€‚
    """
    print(f"\n--- TOOL: ANALYZE SALES DATA ---")
    print(f"ğŸ“ åˆ†æä»»åŠ¡: {question}")
    
    try:
        df = pd.read_csv("sales_data.csv")
        product_sales = df.groupby("äº§å“")["é”€å”®é¢"].sum()
        top_product = product_sales.idxmax()
        top_sales = product_sales.max()
        result = f"{top_product},{top_sales}"
        print(f"ğŸ† åˆ†æç»“æœ: {result}")
        return result
    except Exception as e:
        return f"åˆ†æå¤±è´¥: {e}"

@tool
def write_marketing_copy(product: str, key_points: str) -> str:
    """
    ä¸ºæŒ‡å®šäº§å“æ’°å†™è¥é”€æ–‡æ¡ˆã€‚
    å‚æ•°:
        product (str): éœ€è¦æ’°å†™æ–‡æ¡ˆçš„äº§å“åç§°ã€‚
        key_points (str): æ–‡æ¡ˆéœ€è¦å›´ç»•çš„æ ¸å¿ƒå–ç‚¹ã€‚
    è¿”å›:
        str: ç”Ÿæˆçš„è¥é”€æ–‡æ¡ˆã€‚
    """
    print(f"\n--- TOOL: WRITE MARKETING COPY ---")
    print(f"ğŸ“ æ’°å†™æ–‡æ¡ˆ: {product} - {key_points[:50]}...")
    
    writer_prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè¥é”€æ–‡æ¡ˆã€‚è¯·åŸºäºä»¥ä¸‹äº§å“ä¿¡æ¯æ’°å†™ä¸€ç¯‡ä¸è¶…è¿‡150å­—çš„å¸å¼•äººçš„è¥é”€æ–‡æ¡ˆ:\n"
        "äº§å“åç§°: {product}\n"
        "æ ¸å¿ƒå–ç‚¹: {key_points}\n"
        "æ–‡æ¡ˆ:"
    )
    writer_chain = writer_prompt | model | StrOutputParser()
    
    return writer_chain.invoke({"product": product, "key_points": key_points})

# %%
# --- 4. åˆ›å»ºæ™ºèƒ½ä½“ ---

# Helper function to create a specialist agent
def create_agent(system_prompt: str, tools: list):
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("placeholder", "{messages}"),
    ])
    agent = prompt | model.bind_tools(tools)
    return agent

# åˆ†æå¸ˆæ™ºèƒ½ä½“
analyst_agent = create_agent(
    "ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç»™å®šçš„æ•°æ®å¹¶è¿”å›å…³é”®ç»“æœã€‚è¯·ä½¿ç”¨`analyze_sales_data`å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚",
    [analyze_sales_data]
)

# æ–‡æ¡ˆæ™ºèƒ½ä½“
writer_agent = create_agent(
    "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¥é”€æ–‡æ¡ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®åˆ†æç»“æœï¼Œä¸ºäº§å“æ’°å†™å¼•äººæ³¨ç›®çš„è¥é”€æ–‡æ¡ˆã€‚è¯·ä½¿ç”¨`write_marketing_copy`å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚",
    [write_marketing_copy]
)

# --- 5. å®šä¹‰æ™ºèƒ½ä½“èŠ‚ç‚¹ ---

def analyst_node(state: AgentState):
    print("\n--- CALLING ANALYST AGENT ---")
    result = analyst_agent.invoke({"messages": [HumanMessage(content=state['task'])]})
    return {"messages": [result]}

def writer_node(state: AgentState):
    print("\n--- CALLING WRITER AGENT ---")
    # ä»stateä¸­æå–åˆ†æç»“æœï¼Œå¹¶ä½œä¸ºè¾“å…¥ä¼ é€’ç»™æ–‡æ¡ˆæ™ºèƒ½ä½“
    product, sales = state['analysis_result'].split(',')
    prompt = f"åˆ†æç»“æœï¼šé”€å”®å† å†›æ˜¯â€˜{product}â€™ï¼Œæ€»é”€å”®é¢ä¸º {sales} å…ƒã€‚è¯·ä¸ºæ­¤äº§å“æ’°å†™è¥é”€æ–‡æ¡ˆã€‚"
    result = writer_agent.invoke({"messages": [HumanMessage(content=prompt)]})
    return {"messages": [result]}

# å®šä¹‰å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
tool_node = ToolNode([analyze_sales_data, write_marketing_copy])

def execute_tools(state: AgentState):
    print("\n--- EXECUTING TOOLS ---")
    last_message = state['messages'][-1]
    tool_call = last_message.tool_calls[0]
    
    # æ‰§è¡Œå·¥å…·
    tool_result = tool_node.invoke([last_message])
    
    # æ ¹æ®å·¥å…·æ›´æ–°çŠ¶æ€
    if tool_call['name'] == 'analyze_sales_data':
        return {"messages": tool_result, "analysis_result": tool_result[0].content}
    elif tool_call['name'] == 'write_marketing_copy':
        return {"messages": tool_result, "final_output": tool_result[0].content}
    
    return {"messages": tool_result}


# --- 6. æ„å»ºå›¾ (Supervisoræ¨¡å¼) ---

def supervisor_router(state: AgentState):
    """è·¯ç”±ï¼šå†³å®šä¸‹ä¸€ä¸ªåº”è¯¥ç”±å“ªä¸ªæ™ºèƒ½ä½“æ¥å¤„ç†"""
    print("\n--- SUPERVISOR ---")

    # å¦‚æœåˆ†æç»“æœè¿˜æœªäº§ç”Ÿï¼Œåˆ™åˆ†é…ç»™åˆ†æå¸ˆ
    if not state.get("analysis_result"):
        print("ğŸ“‹ ä»»åŠ¡åˆ†é…: åˆ†æå¸ˆ (Analyst)")
        return "analyst"
        
    # å¦‚æœåˆ†æå·²å®Œæˆä½†æ–‡æ¡ˆè¿˜æœªæ’°å†™ï¼Œåˆ™åˆ†é…ç»™æ–‡æ¡ˆ
    if state.get("analysis_result") and not state.get("final_output"):
        print("ğŸ“‹ ä»»åŠ¡åˆ†é…: æ–‡æ¡ˆæ’°å†™ (Writer)")
        return "writer"
        
    # å¦‚æœä¸€åˆ‡éƒ½å®Œæˆäº†
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
    return END

# æ„å»ºå›¾
isolate_graph = StateGraph(AgentState)

isolate_graph.add_node("analyst", analyst_node)
isolate_graph.add_node("writer", writer_node)
isolate_graph.add_node("execute_tools", execute_tools)

# è®¾ç½®å…¥å£ç‚¹
isolate_graph.set_entry_point("analyst")

# å®šä¹‰å›¾çš„è¾¹
isolate_graph.add_edge("analyst", "execute_tools")
isolate_graph.add_edge("writer", "execute_tools")
isolate_graph.add_conditional_edges(
    "execute_tools",
    supervisor_router,
    {"analyst": "analyst", "writer": "writer", END: END}
)

# ç¼–è¯‘å·¥ä½œæµ
isolate_workflow = isolate_graph.compile()

# --- 7. æ‰§è¡Œå¤šæ™ºèƒ½ä½“åä½œ ---
print("\n### æ¼”ç¤ºå¤šæ™ºèƒ½ä½“åä½œ (Supervisoræ¨¡å¼) ###")
task = (
    "åˆ†æé”€å”®æ•°æ®æ‰¾å‡ºé”€å”®é¢æœ€é«˜çš„äº§å“ï¼Œ"
    "ç„¶åä¸ºè¯¥äº§å“æ’°å†™ä¸€ç¯‡å¸å¼•äººçš„è¥é”€æ–‡æ¡ˆã€‚"
)
initial_state = AgentState(
    messages=[],
    task=task,
    analysis_result="",
    final_output="",
    next_agent="analyst"
)

# æ‰§è¡Œå·¥ä½œæµ
for step in isolate_workflow.stream(initial_state, {"recursion_limit": 10}):
    node = list(step.keys())[0]
    state = step[node]
    print(f"--- [{node}] æ­¥éª¤å®Œæˆ ---")
    if "final_output" in state and state["final_output"]:
        print(f"\nğŸ‰ æœ€ç»ˆæ–‡æ¡ˆ:\n{state['final_output']}")


