{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e2281b",
   "metadata": {},
   "source": [
    "# **使用 LangGraph 和 Qwen 模型实现上下文工程四大策略**\n",
    "\n",
    "### **目标**\n",
    "本 Notebook 将作为一份详细的技术指南，演示如何使用 LangGraph 框架和通义千问（Qwen）模型，一步步实现“上下文工程”的四大核心策略。\n",
    "\n",
    "四大策略包括：\n",
    "1.  **写入 (Write):** 为智能体构建一个“外部大脑”（暂存区），以在长任务中保持状态。\n",
    "2.  **选择 (Select):** 使用检索增强生成（RAG）从知识库中精准调取信息。\n",
    "3.  **压缩 (Compress):** 智能地总结对话历史，以节省成本和Token。\n",
    "4.  **隔离 (Isolate):** 使用多智能体（Multi-agent）架构，将复杂任务分解给专家处理。\n",
    "\n",
    "---\n",
    "### **第一步：环境设置与模型初始化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddefae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\n",
      "Collecting langchain\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/f1/f2/c09a2e383283e3af1db669ab037ac05a45814f4b9c472c48dc24c0cef039/langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-qwq\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/06/f8/3f0419be750e897a4c4ebf932096722216ac43512b25aedae05f916cbd47/langchain_qwq-0.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting langgraph\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/d7/2f/11be9302d3a213debcfe44355453a1e8fd7ee5e3138edeb8bd82b56bc8f6/langgraph-0.5.3-py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m903.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/07/5f/63760ff107bcf5146eee41b38b3985f9055e710a72fdd637b791dea3495c/pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/5f/ed/539768cf28c661b5b068d66d96a2f155c4971a5d55684a514c1a0e0dec2f/python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/c8/bc/f8c7dae8321d37ed39ac9d7896617c4203248240a4835b136e3724b3bb62/langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dashscope\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/61/3f/2d1e656e997ddfaf3a3fde74d9b5120689338e4435ecc26b5c95720a6dc9/dashscope-1.23.9-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting faiss-cpu\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/84/30/e06cfcedf4664907f39a93f21988149f05ae7fef62e988abb9e99940beeb/faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/51/7b/bb7b088440ff9cc55e9e6eba94162cbdcd3b1693c194e1ad4764acba29b9/langchain_core-0.3.69-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.6/441.6 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/8b/a3/3696ff2444658053c01b6b7443e761f28bb71217d82bb89137a978c5f66f/langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/19/4f/481324462c44ce21443b833ad73ee51117031d41c16fec06cddbb7495b26/langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.0/368.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/6a/c0/ec2b1c8712ca690e5d61979dee872603e92b8a32f94cc1b72d53beab008a/pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/62/e4/b9a7a0e5c6f79d49bcd6efb6e90d7536dc604dab64582a9dec220dab54b6/sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests<3,>=2 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/75/e4/2c27590dfc9992f73aabbeb9241ae20220bd9452df27483b6e56d3975cc5/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting json-repair<0.41.0,>=0.40.0 (from langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/f4/e1/f0e63cc027669763ccc2c1e62ba69959ec02db5328c81df2508a52711ec9/json_repair-0.40.0-py3-none-any.whl (20 kB)\n",
      "Collecting langchain-openai<0.4.0,>=0.3.11 (from langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/91/56/75f3d84b69b8bdae521a537697375e1241377627c32b78edcae337093502/langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m867.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openai<2.0.0,>=1.70.0 (from langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/8a/91/1f1cf577f745e956b276a8b1d3d76fa7a6ee0c2b05db3b001b900f2c71db/openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/4c/dd/64686797b0927fb18b290044be12ae9d4df01670dce6bb2498d5ab65cb24/langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/c3/64/6bc45ab9e0e1112698ebff579fe21f5606ea65cd08266995a357e312a4d2/langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/77/86/56e01e715e5b0028cdaff1492a89e54fa12e18c21e03b805a10ea36ecd5a/langgraph_sdk-0.1.73-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m467.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash>=3.5.0 (from langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/d9/72/9256303f10e41ab004799a4aa74b80b3c5977d6383ae4550548b24bd1971/xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m661.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/75/c9/9bec03675192077467a9c7c2bdd1f2e922bd01d3a69b15c3a0fdcd8548f6/numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /root/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/e0/30/aadcdf71b510a718e3d98a7bfeaea2396ac847f218b7e8edb241b09bd99a/aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/e5/30/643397144bfbfec6f6ef821f36f33e57d35946c44a2352d3c9f0ae847619/tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/58/f0/427018098906416f580e3cf1366d3b1abfb408a0652e9f31600c24a1903c/pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/25/0a/6269e3473b09aed2dab8aa1a600c70f31f00ae1349bee30658f7e358a159/httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Collecting websocket-client (from dashscope)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/5a/84/44687a29792a70e111c5c477230a72c4b957d88d16141199bf9acb7537a3/websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography (from dashscope)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/c9/d8/0749f7d39f53f8258e5c18a93131919ac465ee1f9dccaf1b3f420235e0b5/cryptography-45.0.5-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /root/.local/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/fb/76/641ae371508676492379f16e2fa48f4e2c11741bd63c48be4b12a6b09cba/aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/4d/83/220a374bd7b2aeba9d0725130665afe11de347d95c3620b9b82cc2fcab97/frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/74/07/2c9246cda322dfe08be85f1b8739646f2c4c5113a1422d7a407763422ec4/multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.6/246.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/5d/e6/116ba39448753b1330f48ab8ba927dcd6cf0baea8a0ccbc512dfb49ba670/propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.5/213.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/00/70/8f78a95d6935a70263d46caa3dd18e1f223cf2f2ff2037baa01a22bc5b22/yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.0/349.0 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/34/75/51952c7b2d3873b44a0028b1bd26a25078c18f92f256608e8d1dc61b39fd/marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /root/.local/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai<0.4.0,>=0.3.11->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/b1/73/41591c525680cd460a6becf56c9b17468d3711b1df242c53d2c7b2183d16/tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/4e/92/7c91e8115fc37e88d1a35e13200fda3054ff5d2e5adf017345e58cea4834/ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.25.2 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/67/4f/d22f79a3c56dde563c4fbc12eebf9224a1b87af5e4ec61beb11f9b3eb499/orjson-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m776.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/76/3f/dbafccf19cfeca25bbabf6f2dd81796b7218f768ec400f043edc767015a6/zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.70.0->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.70.0->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.70.0->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/81/5a/0e73541b6edd3f4aada586c24e50626c7815c561a7ba337d6a7eb0a915b4/jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai<2.0.0,>=1.70.0->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.70.0->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/47/bc/cd720e078576bdb8255d5032c5d63ee5c0bf4b7173dd955185a1d658c456/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in /root/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/92/9b/ad67f03d74554bed3aefd56fe836e1623a50780f7c998d00ca128924a499/charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/4f/52/34c6cf5bb9285074dc3531c437b3919e825d976fde097a7a73f79e726d03/certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/65/89/77acf9e3da38e9bcfca881e43b02ed467c1dedc387021fc4d9bd9928afb8/greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cffi>=1.14 (from cryptography->dashscope)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/ff/6b/d45873c5e0242196f042d555526f92aa9e0c32355a1be1ff8c27f077fd37/cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (467 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycparser (from cffi>=1.14->cryptography->dashscope)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/71/92/5e77f98553e9e75130c78900d000368476aed74276eb8ae8796f65f00918/jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai<0.4.0,>=0.3.11->langchain-qwq)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/bf/ce/0d0e61429f603bac433910d99ef1a02ce45a8967ffbe3cbee48599e62d88/regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: pytz, zstandard, xxhash, websocket-client, urllib3, tzdata, typing-inspection, tqdm, tenacity, sniffio, regex, PyYAML, python-dotenv, pydantic-core, pycparser, propcache, ormsgpack, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, json-repair, jiter, idna, httpx-sse, h11, greenlet, frozenlist, distro, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, pandas, jsonpatch, httpcore, faiss-cpu, cffi, anyio, aiosignal, tiktoken, requests-toolbelt, pydantic-settings, httpx, dataclasses-json, cryptography, aiohttp, openai, langsmith, langgraph-sdk, dashscope, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain-qwq, langchain, langgraph, langchain_community\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 certifi-2025.7.14 cffi-1.17.1 charset_normalizer-3.4.2 cryptography-45.0.5 dashscope-1.23.9 dataclasses-json-0.6.7 distro-1.9.0 faiss-cpu-1.11.0.post1 frozenlist-1.7.0 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 idna-3.10 jiter-0.10.0 json-repair-0.40.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.26 langchain-core-0.3.69 langchain-openai-0.3.28 langchain-qwq-0.2.0 langchain-text-splitters-0.3.8 langchain_community-0.3.27 langgraph-0.5.3 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.73 langsmith-0.4.8 marshmallow-3.26.1 multidict-6.6.3 mypy-extensions-1.1.0 numpy-2.3.1 openai-1.97.0 orjson-3.11.0 ormsgpack-1.10.0 pandas-2.3.1 propcache-0.3.2 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 pytz-2025.2 regex-2024.11.6 requests-2.32.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.9.0 tqdm-4.67.1 typing-inspect-0.9.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 websocket-client-1.8.0 xxhash-3.5.0 yarl-1.20.1 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/python3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain-qwq langgraph pandas python-dotenv langchain_community dashscope faiss-cpu pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0417ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import operator\n",
    "from typing import List, TypedDict, Annotated\n",
    "\n",
    "import tiktoken\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_qwq import ChatQwen\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1547b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 设置API密钥 ---\n",
    "# 请注意：这里需要的是 DashScope 的 API Key\n",
    "if \"DASHSCOPE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"DASHSCOPE_API_KEY\"] = getpass.getpass(\"请输入您的DashScope API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5d7428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen 模型初始化成功！\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 初始化Qwen模型 ---\n",
    "# 我们将使用 qwen3-32b 模型作为我们智能体的“大脑”\n",
    "try:\n",
    "    model = ChatQwen(model=\"qwen3-30b-a3b\", base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  enable_thinking=False)\n",
    "    print(\"Qwen 模型初始化成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"模型初始化失败，请检查API Key或网络连接: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc73c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 初始化Token计算器 ---\n",
    "# 这将帮助我们量化“压缩”策略带来的效果\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80f1e5",
   "metadata": {},
   "source": [
    "## **策略一：写入 (Write) - 构建智能体的“草稿纸”**\n",
    "\n",
    "**核心思想:** 不把所有中间步骤和思考都塞进主对话历史（`messages`），而是将它们“写入”到一个独立的“暂存区”（`scratchpad`）。这可以保持主对话的清晰，并为智能体提供一个可靠的短期记忆，防止在长任务中“失忆”。\n",
    "\n",
    "**实现:** 我们将在`AgentState`中增加一个`scratchpad`字段，并通过添加`SystemMessage`来指导模型的行为，防止无限循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a46ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 定义状态 ---\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "import operator\n",
    "\n",
    "class ToolCallRecord(TypedDict):\n",
    "    step: int\n",
    "    tool_name: str\n",
    "    args: dict\n",
    "    result: str\n",
    "\n",
    "class WriteStrategyState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    # 结构化 scratchpad，保留完整历史\n",
    "    scratchpad: dict  # {\"history\": List[ToolCallRecord], \"final_answer\": str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436e02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 定义工具 ---\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def simple_calculator(operation: str, a: int, b: int) -> int:\n",
    "    \"\"\"一个简单的计算器工具，执行加减乘除。\"\"\"\n",
    "    if operation == \"add\":\n",
    "        return a + b\n",
    "    if operation == \"subtract\":\n",
    "        return a - b\n",
    "    if operation == \"multiply\":\n",
    "        return a * b\n",
    "    if operation == \"divide\" and b != 0:\n",
    "        return a // b\n",
    "    return \"无效操作\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d01195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 定义图的节点 ---\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import END\n",
    "\n",
    "tools = [simple_calculator]\n",
    "tool_node = ToolNode(tools)\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "def agent_with_scratchpad(state: WriteStrategyState):\n",
    "    \"\"\"\n",
    "    Agent 节点：决定下一步动作，并更新暂存区。\n",
    "    \"\"\"\n",
    "    print(\"---AGENT NODE---\")\n",
    "    response = model_with_tools.invoke(state['messages'])\n",
    "\n",
    "    if response.tool_calls:\n",
    "        # 暂存当前待执行的工具\n",
    "        state['scratchpad']['pending_tool'] = response.tool_calls[0]\n",
    "        print(f\"🧠 Agent Action: Call tool `{response.tool_calls[0]['name']}` \"\n",
    "              f\"with arguments `{response.tool_calls[0]['args']}`\")\n",
    "    else:\n",
    "        # 全部完成，保存最终答案\n",
    "        state['scratchpad']['final_answer'] = response.content\n",
    "        print(f\"✅ Final Answer: {response.content}\")\n",
    "\n",
    "    return {\"messages\": [response], \"scratchpad\": state['scratchpad']}\n",
    "\n",
    "def tool_node_with_scratchpad(state: WriteStrategyState):\n",
    "    \"\"\"\n",
    "    Tool 节点：执行工具，并把结果记录到 history。\n",
    "    \"\"\"\n",
    "    print(\"---TOOL NODE---\")\n",
    "    last_message = state['messages'][-1]\n",
    "    tool_messages = tool_node.invoke([last_message])\n",
    "\n",
    "    # 取出待处理的工具调用\n",
    "    pending = state['scratchpad']['pending_tool']\n",
    "    record = ToolCallRecord(\n",
    "        step=len(state['scratchpad'].get(\"history\", [])) + 1,\n",
    "        tool_name=pending['name'],\n",
    "        args=pending['args'],\n",
    "        result=str(tool_messages[0].content)\n",
    "    )\n",
    "    # 追加到历史\n",
    "    state['scratchpad'].setdefault(\"history\", []).append(record)\n",
    "    print(f\"📝 Recorded Tool Call: {record}\")\n",
    "    state['scratchpad'].pop(\"pending_tool\", None)  # 清理\n",
    "\n",
    "    #print(f\"🛠️ Tool Result: `{record['result']}`\")\n",
    "    return {\"messages\": tool_messages, \"scratchpad\": state['scratchpad']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be3a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 构建图 ---\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "write_graph_builder = StateGraph(WriteStrategyState)\n",
    "write_graph_builder.add_node(\"agent\", agent_with_scratchpad)\n",
    "write_graph_builder.add_node(\"action\", tool_node_with_scratchpad)\n",
    "write_graph_builder.set_entry_point(\"agent\")\n",
    "\n",
    "# 条件边：检查是否还有未完成的工具\n",
    "def should_continue(state: WriteStrategyState) -> str:\n",
    "    # 若最终答案已存在，直接结束\n",
    "    if state['scratchpad'].get(\"final_answer\"):\n",
    "        return END\n",
    "    return \"action\"\n",
    "\n",
    "write_graph_builder.add_conditional_edges(\"agent\", should_continue, {\"action\": \"action\", END: END})\n",
    "write_graph_builder.add_edge(\"action\", \"agent\")\n",
    "write_graph = write_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa372ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 演示“写入”策略 ###\n",
      "---AGENT NODE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Agent Action: Call tool `simple_calculator` with arguments `{'a': 128, 'b': 72, 'operation': 'add'}`\n",
      "---\n",
      "---TOOL NODE---\n",
      "📝 Recorded Tool Call: {'step': 1, 'tool_name': 'simple_calculator', 'args': {'a': 128, 'b': 72, 'operation': 'add'}, 'result': '200'}\n",
      "---\n",
      "---AGENT NODE---\n",
      "🧠 Agent Action: Call tool `simple_calculator` with arguments `{'a': 200, 'b': 3, 'operation': 'multiply'}`\n",
      "---\n",
      "---TOOL NODE---\n",
      "📝 Recorded Tool Call: {'step': 2, 'tool_name': 'simple_calculator', 'args': {'a': 200, 'b': 3, 'operation': 'multiply'}, 'result': '600'}\n",
      "---\n",
      "---AGENT NODE---\n",
      "🧠 Agent Action: Call tool `simple_calculator` with arguments `{'a': 600, 'b': 100, 'operation': 'divide'}`\n",
      "---\n",
      "---TOOL NODE---\n",
      "📝 Recorded Tool Call: {'step': 3, 'tool_name': 'simple_calculator', 'args': {'a': 600, 'b': 100, 'operation': 'divide'}, 'result': '6'}\n",
      "---\n",
      "---AGENT NODE---\n",
      "🧠 Agent Action: Call tool `simple_calculator` with arguments `{'a': 6, 'b': 20, 'operation': 'multiply'}`\n",
      "---\n",
      "---TOOL NODE---\n",
      "📝 Recorded Tool Call: {'step': 4, 'tool_name': 'simple_calculator', 'args': {'a': 6, 'b': 20, 'operation': 'multiply'}, 'result': '120'}\n",
      "---\n",
      "---AGENT NODE---\n",
      "🧠 Agent Action: Call tool `simple_calculator` with arguments `{'a': 120, 'b': 222, 'operation': 'subtract'}`\n",
      "---\n",
      "---TOOL NODE---\n",
      "📝 Recorded Tool Call: {'step': 5, 'tool_name': 'simple_calculator', 'args': {'a': 120, 'b': 222, 'operation': 'subtract'}, 'result': '-102'}\n",
      "---\n",
      "---AGENT NODE---\n",
      "✅ Final Answer: 最终结果：-102\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 演示 ---\n",
    "print(\"### 演示“写入”策略 ###\")\n",
    "task = (\n",
    "    \"1) 初始现金流 128 元与预算追加 72 元先进行合并；\\n\"\n",
    "    \"2) 合并后的资金按季度复利 3 倍杠杆放大；\\n\"\n",
    "    \"3) 放大后的资金因汇率折算需除以 100 得到基准单位值；\\n\"\n",
    "    \"4) 基准单位值再按 20 倍风险系数放大，形成风险敞口；\\n\"\n",
    "    \"5) 最终从风险敞口中一次性扣除 222 元的固定准备金。\\n\"\n",
    "    \"请列出每一步的数值结果，并以『最终结果：{数值}』的格式给出答案。\"\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"你是一个计算助手。请按步骤使用 `simple_calculator` 工具来回答用户的问题。 \"\n",
    ")\n",
    "initial_messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=task)\n",
    "]\n",
    "initial_state = {\"messages\": initial_messages, \"scratchpad\": {\"history\": [], \"final_answer\": None}}\n",
    "\n",
    "# 使用 .stream() 观察每一步\n",
    "for step in write_graph.stream(initial_state, {\"recursion_limit\": 20}):\n",
    "    #print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e126c53",
   "metadata": {},
   "source": [
    "## **策略二：选择 (Select) - 精准的\"信息调取\"**\n",
    "\n",
    "**核心思想：** 使用RAG（检索增强生成）技术，从外部知识库中精准检索最相关的信息片段，只将必要信息注入上下文。\n",
    "\n",
    "**实现步骤：**\n",
    "1. 创建产品知识库（模拟向量数据库）\n",
    "2. 构建RAG检索器\n",
    "3. 设计智能体流程：问题 → 检索 → 生成答案\n",
    "4. 可视化Token节省效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d67324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 创建模拟产品知识库 ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "# 创建嵌入模型\n",
    "embeddings = DashScopeEmbeddings(model=\"text-embedding-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f0b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产品知识文档（实际应用中会从数据库加载）\n",
    "product_docs = [\n",
    "    Document(page_content=\"\"\"机械键盘 X1 Pro 技术规格：\n",
    "- 轴体：定制青轴，60g触发压力\n",
    "- 连接：三模（蓝牙5.1/2.4G/USB-C）\n",
    "- 电池：4000mAh，续航200小时\n",
    "- 特点：热插拔轴体，PBT双色键帽，全键无冲\n",
    "- 价格：699元（限时优惠599元）\"\"\", \n",
    "             metadata={\"product\": \"机械键盘 X1 Pro\", \"category\": \"键盘\"}),\n",
    "    \n",
    "    Document(page_content=\"\"\"游戏鼠标 M800 旗舰版：\n",
    "- 传感器：原相PAW3395，26000DPI\n",
    "- 微动：欧姆龙光学微动，1亿次寿命\n",
    "- 重量：58g（超轻量化设计）\n",
    "- RGB：1680万色，10区域独立控光\n",
    "- 价格：399元（套装优惠价）\"\"\", \n",
    "             metadata={\"product\": \"游戏鼠标 M800\", \"category\": \"鼠标\"}),\n",
    "    \n",
    "    Document(page_content=\"\"\"促销邮件写作指南：\n",
    "1. 标题要吸引眼球，包含优惠信息\n",
    "2. 开头用痛点场景引发共鸣\n",
    "3. 突出产品核心优势（性能>参数）\n",
    "4. 限时优惠制造紧迫感\n",
    "5. 清晰的行动召唤按钮\"\"\", \n",
    "             metadata={\"doc_type\": \"writing_guide\"}),\n",
    "    \n",
    "    Document(page_content=\"\"\"用户偏好分析：\n",
    "科技产品消费者最关注：\n",
    "- 性能参数（75%用户）\n",
    "- 性价比（68%用户）\n",
    "- 耐用性（52%用户）\n",
    "- 外观设计（48%用户）\"\"\", \n",
    "             metadata={\"doc_type\": \"user_insight\"}),\n",
    "]\n",
    "\n",
    "# 创建向量数据库\n",
    "vector_db = FAISS.from_documents(product_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259408e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 构建RAG检索器 ---\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 创建检索器\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"格式化检索到的文档\"\"\"\n",
    "    return \"\\n\\n\".join(f\"## 来源 {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs))\n",
    "\n",
    "# 创建RAG提示模板\n",
    "rag_template = \"\"\"\n",
    "你是一位专业的产品文案助手。请根据提供的背景信息回答用户问题。\n",
    "\n",
    "<背景信息>\n",
    "{context}\n",
    "</背景信息>\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "请用专业、简洁的语言回答，突出产品核心优势：\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "# 创建RAG链\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b04e7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 设计智能体流程 ---\n",
    "class SelectStrategyState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    context: str  # 存储检索到的上下文\n",
    "\n",
    "def retrieve_context(state: SelectStrategyState):\n",
    "    \"\"\"检索节点：从知识库获取相关信息\"\"\"\n",
    "    print(\"\\n--- RETRIEVE CONTEXT ---\")\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    # 执行检索\n",
    "    docs = retriever.invoke(last_message)\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # 计算Token节省\n",
    "    orig_token_count = sum(len(encoding.encode(doc.page_content)) for doc in docs)\n",
    "    context_token_count = len(encoding.encode(context))\n",
    "    savings = orig_token_count - context_token_count\n",
    "    \n",
    "    print(f\"🔍 检索到 {len(docs)} 条相关文档\")\n",
    "    print(f\"📉 Token节省: {savings} (原始: {orig_token_count} -> 压缩: {context_token_count})\")\n",
    "    print(f\"📝 注入上下文:\\n{context[:300]}...\")\n",
    "    \n",
    "    return {\"context\": context}\n",
    "\n",
    "def generate_with_context(state: SelectStrategyState):\n",
    "    \"\"\"生成节点：使用检索到的上下文生成回答\"\"\"\n",
    "    print(\"\\n--- GENERATE WITH CONTEXT ---\")\n",
    "    question = state[\"messages\"][-1].content\n",
    "    \n",
    "    # 使用RAG链生成回答\n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    # 创建消息对象\n",
    "    response_message = HumanMessage(content=response)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"💡 生成的回答: {response}\")\n",
    "    return {\"messages\": [response_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58e344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 构建选择策略图 ---\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# 定义状态\n",
    "class SelectState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    context: str\n",
    "\n",
    "# 创建图\n",
    "select_graph = StateGraph(SelectState)\n",
    "\n",
    "# 添加节点\n",
    "select_graph.add_node(\"retrieve\", retrieve_context)\n",
    "select_graph.add_node(\"generate\", generate_with_context)\n",
    "\n",
    "# 设置入口点\n",
    "select_graph.set_entry_point(\"retrieve\")\n",
    "\n",
    "# 添加边\n",
    "select_graph.add_edge(\"retrieve\", \"generate\")\n",
    "select_graph.add_edge(\"generate\", END)\n",
    "\n",
    "# 编译图\n",
    "select_workflow = select_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd425e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 演示'选择'策略 ###\n",
      "\n",
      "--- RETRIEVE CONTEXT ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 检索到 2 条相关文档\n",
      "📉 Token节省: -14 (原始: 223 -> 压缩: 237)\n",
      "📝 注入上下文:\n",
      "## 来源 1:\n",
      "机械键盘 X1 Pro 技术规格：\n",
      "- 轴体：定制青轴，60g触发压力\n",
      "- 连接：三模（蓝牙5.1/2.4G/USB-C）\n",
      "- 电池：4000mAh，续航200小时\n",
      "- 特点：热插拔轴体，PBT双色键帽，全键无冲\n",
      "- 价格：699元（限时优惠599元）\n",
      "\n",
      "## 来源 2:\n",
      "促销邮件写作指南：\n",
      "1. 标题要吸引眼球，包含优惠信息\n",
      "2. 开头用痛点场景引发共鸣\n",
      "3. 突出产品核心优势（性能>参数）\n",
      "4. 限时优惠制造紧迫感\n",
      "5. 清晰的行动召唤按钮...\n",
      "{'retrieve': {'context': '## 来源 1:\\n机械键盘 X1 Pro 技术规格：\\n- 轴体：定制青轴，60g触发压力\\n- 连接：三模（蓝牙5.1/2.4G/USB-C）\\n- 电池：4000mAh，续航200小时\\n- 特点：热插拔轴体，PBT双色键帽，全键无冲\\n- 价格：699元（限时优惠599元）\\n\\n## 来源 2:\\n促销邮件写作指南：\\n1. 标题要吸引眼球，包含优惠信息\\n2. 开头用痛点场景引发共鸣\\n3. 突出产品核心优势（性能>参数）\\n4. 限时优惠制造紧迫感\\n5. 清晰的行动召唤按钮'}}\n",
      "---\n",
      "\n",
      "--- GENERATE WITH CONTEXT ---\n",
      "💡 生成的回答: **主题：限时特惠！旗舰机械键盘X1 Pro，性能升级，仅需599元**\n",
      "\n",
      "尊敬的用户，\n",
      "\n",
      "你是否在寻找一款兼具专业性能与舒适体验的机械键盘？无论是高强度办公、游戏竞技，还是长时间打字，X1 Pro 都能为你带来卓越的使用感受。\n",
      "\n",
      "**X1 Pro 核心优势：**  \n",
      "✅ **定制青轴设计**：60g触发压力，清晰回弹，敲击感精准有力。  \n",
      "✅ **三模连接技术**：支持蓝牙5.1/2.4G/USB-C，兼容多种设备，无缝切换。  \n",
      "✅ **持久续航能力**：4000mAh大电池，续航长达200小时，告别频繁充电。  \n",
      "✅ **热插拔轴体 + PBT双色键帽**：自由更换轴体，耐用性强，质感出众。  \n",
      "✅ **全键无冲**：游戏场景中精准识别多键输入，提升操作稳定性。\n",
      "\n",
      "**现在购买，立享限时优惠价：599元（原价699元）**  \n",
      "机会有限，错过再等一年！\n",
      "\n",
      "👉 [立即抢购]  \n",
      "\n",
      "让X1 Pro 成为你工作与娱乐的最佳搭档，点击下方按钮，马上享受超值优惠！\n",
      "{'generate': {'messages': [HumanMessage(content='**主题：限时特惠！旗舰机械键盘X1 Pro，性能升级，仅需599元**\\n\\n尊敬的用户，\\n\\n你是否在寻找一款兼具专业性能与舒适体验的机械键盘？无论是高强度办公、游戏竞技，还是长时间打字，X1 Pro 都能为你带来卓越的使用感受。\\n\\n**X1 Pro 核心优势：**  \\n✅ **定制青轴设计**：60g触发压力，清晰回弹，敲击感精准有力。  \\n✅ **三模连接技术**：支持蓝牙5.1/2.4G/USB-C，兼容多种设备，无缝切换。  \\n✅ **持久续航能力**：4000mAh大电池，续航长达200小时，告别频繁充电。  \\n✅ **热插拔轴体 + PBT双色键帽**：自由更换轴体，耐用性强，质感出众。  \\n✅ **全键无冲**：游戏场景中精准识别多键输入，提升操作稳定性。\\n\\n**现在购买，立享限时优惠价：599元（原价699元）**  \\n机会有限，错过再等一年！\\n\\n👉 [立即抢购]  \\n\\n让X1 Pro 成为你工作与娱乐的最佳搭档，点击下方按钮，马上享受超值优惠！', additional_kwargs={}, response_metadata={})]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 演示选择策略 ---\n",
    "print(\"\\n### 演示'选择'策略 ###\")\n",
    "question = \"请为我们的旗舰机械键盘X1 Pro写一封促销邮件，突出其核心优势\"\n",
    "\n",
    "# 初始状态\n",
    "initial_state = SelectState(\n",
    "    messages=[HumanMessage(content=question)],\n",
    "    context=\"\"\n",
    ")\n",
    "\n",
    "# 执行工作流\n",
    "for step in select_workflow.stream(initial_state):\n",
    "    if \"__end__\" not in step:\n",
    "        print(step)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78e84",
   "metadata": {},
   "source": [
    "## **策略三：压缩 (Compress) - 为上下文\"瘦身减负\"**\n",
    "\n",
    "**核心思想：** 使用总结(summarization)和裁剪(trimming)技术减少上下文长度，节省Token并提高效率。\n",
    "\n",
    "**实现两种压缩技术：**\n",
    "1. **总结压缩**：将长文本提炼为简洁摘要\n",
    "2. **裁剪压缩**：智能保留对话中最相关的部分\n",
    "\n",
    "**场景演示：** 智能体需要阅读一篇长文章并回答问题，我们通过总结压缩文章内容；同时展示对话历史裁剪技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1022bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 准备长文本示例 ---\n",
    "long_article = \"\"\"\n",
    "在人工智能领域，大语言模型（LLM）的发展正以前所未有的速度推进。2023年，OpenAI发布了GPT-4模型，其上下文窗口扩展到32K tokens，大大增强了处理长文档的能力。随后，Anthropic推出了Claude 2.1模型，支持200K tokens的上下文窗口，创下了当时的新纪录。\n",
    "\n",
    "然而，2024年，这一纪录被中国科技公司深度求索（DeepSeek）打破。他们发布了DeepSeek-R1模型，不仅支持128K tokens的上下文窗口，还创新性地引入了\"上下文压缩\"技术。该技术通过智能总结和关键信息提取，可以将长文档压缩到原长度的20%-30%，同时保留95%以上的核心信息。\n",
    "\n",
    "DeepSeek-R1的技术创新主要体现在三个方面：\n",
    "1. 分层总结架构：模型首先对文档进行分段总结，然后对分段摘要进行二次总结，形成层次化的压缩结构。\n",
    "2. 语义密度优化：通过强化学习训练，模型学会识别并保留信息密度最高的内容。\n",
    "3. 自适应压缩率：根据用户任务类型动态调整压缩强度，平衡信息保留与效率。\n",
    "\n",
    "在实际测试中，DeepSeek-R1处理一篇10,000字的科技论文时，将其压缩到1,500字的关键摘要，同时准确回答了论文中的核心问题。更令人印象深刻的是，压缩后的Token使用量仅为原始的18%，而任务完成质量仅下降2%。\n",
    "\n",
    "这项技术的商业应用前景广阔：\n",
    "- 法律行业：快速分析冗长的法律文件\n",
    "- 金融领域：高效处理年度财报和招股书\n",
    "- 学术研究：加速文献综述过程\n",
    "- 客户服务：快速理解长篇客户反馈\n",
    "\n",
    "DeepSeek团队表示，他们下一步将探索\"动态上下文压缩\"，即在对话过程中实时调整压缩率，进一步优化智能体的长期记忆管理。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e07de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 定义压缩工具 ---\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 总结压缩工具\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"请将以下文本总结为不超过{max_words}字的关键要点，保留所有核心技术和数据：\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "summarizer_chain = (\n",
    "    summary_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 裁剪压缩函数\n",
    "def trim_messages(messages: List[BaseMessage], max_messages=5) -> List[BaseMessage]:\n",
    "    \"\"\"裁剪对话历史，保留系统消息和最新的几条消息\"\"\"\n",
    "    # 始终保留第一条系统消息\n",
    "    system_message = messages[0] if messages and isinstance(messages[0], SystemMessage) else None\n",
    "    \n",
    "    # 保留最近的max_messages条消息（排除系统消息）\n",
    "    recent_messages = messages[-max_messages:] if len(messages) > 1 else messages\n",
    "    \n",
    "    # 重新组合\n",
    "    trimmed = []\n",
    "    if system_message:\n",
    "        trimmed.append(system_message)\n",
    "    trimmed.extend(recent_messages)\n",
    "    \n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59937644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 定义状态和节点 ---\n",
    "class CompressState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    original_text: str  # 原始长文本\n",
    "    compressed_text: str  # 压缩后的文本\n",
    "    token_savings: int  # 节省的Token数量\n",
    "\n",
    "def compress_long_text(state: CompressState):\n",
    "    \"\"\"总结压缩节点：将长文本压缩为摘要\"\"\"\n",
    "    print(\"\\n--- COMPRESSING LONG TEXT ---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # 从用户消息中提取问题\n",
    "    question = last_message.content\n",
    "    \n",
    "    # 压缩长文本\n",
    "    summary = summarizer_chain.invoke({\"text\": state[\"original_text\"], \"max_words\": 300})\n",
    "    \n",
    "    # 计算Token节省\n",
    "    orig_tokens = len(encoding.encode(state[\"original_text\"]))\n",
    "    comp_tokens = len(encoding.encode(summary))\n",
    "    savings = orig_tokens - comp_tokens\n",
    "    \n",
    "    print(f\"📉 文本压缩: {orig_tokens} tokens → {comp_tokens} tokens (节省 {savings} tokens)\")\n",
    "    print(f\"📝 压缩摘要:\\n{summary[:200]}...\")\n",
    "    \n",
    "    # 更新状态\n",
    "    return {\n",
    "        \"compressed_text\": summary,\n",
    "        \"token_savings\": savings,\n",
    "        \"messages\": [HumanMessage(content=f\"基于以下摘要回答问题:\\n{summary}\\n\\n问题: {question}\")]\n",
    "    }\n",
    "\n",
    "def answer_with_compressed_text(state: CompressState):\n",
    "    \"\"\"回答节点：基于压缩文本回答问题\"\"\"\n",
    "    print(\"\\n--- ANSWERING WITH COMPRESSED TEXT ---\")\n",
    "    \n",
    "    # 调用模型生成答案\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    answer = response.content\n",
    "    \n",
    "    print(f\"💡 生成的回答: {answer[:200]}...\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def trim_context(state: CompressState):\n",
    "    \"\"\"裁剪节点：压缩对话历史\"\"\"\n",
    "    print(\"\\n--- TRIMMING CONTEXT ---\")\n",
    "    \n",
    "    # 计算裁剪前的Token\n",
    "    all_messages = \"\".join(m.content for m in state[\"messages\"])\n",
    "    before_tokens = len(encoding.encode(all_messages))\n",
    "    \n",
    "    # 执行裁剪\n",
    "    trimmed_messages = trim_messages(state[\"messages\"], max_messages=3)\n",
    "    \n",
    "    # 计算裁剪后的Token\n",
    "    trimmed_content = \"\".join(m.content for m in trimmed_messages)\n",
    "    after_tokens = len(encoding.encode(trimmed_content))\n",
    "    savings = before_tokens - after_tokens\n",
    "    \n",
    "    print(f\"✂️ 裁剪历史: {len(state['messages'])}条 → {len(trimmed_messages)}条消息\")\n",
    "    print(f\"📉 Token节省: {savings} (原始: {before_tokens} -> 裁剪后: {after_tokens})\")\n",
    "    \n",
    "    return {\"messages\": trimmed_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "047924db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 构建压缩策略图 ---\n",
    "compress_graph = StateGraph(CompressState)\n",
    "\n",
    "# 添加节点\n",
    "compress_graph.add_node(\"compress\", compress_long_text)\n",
    "compress_graph.add_node(\"answer\", answer_with_compressed_text)\n",
    "compress_graph.add_node(\"trim\", trim_context)\n",
    "\n",
    "# 设置入口点\n",
    "compress_graph.set_entry_point(\"compress\")\n",
    "\n",
    "# 添加边\n",
    "compress_graph.add_edge(\"compress\", \"answer\")\n",
    "compress_graph.add_edge(\"answer\", END)\n",
    "\n",
    "# 添加条件边用于裁剪\n",
    "def should_trim(state: CompressState):\n",
    "    \"\"\"当消息超过5条时触发裁剪\"\"\"\n",
    "    if len(state[\"messages\"]) > 5:\n",
    "        return \"trim\"\n",
    "    return END\n",
    "\n",
    "compress_graph.add_conditional_edges(\"answer\", should_trim, {\"trim\": \"trim\", END: END})\n",
    "compress_graph.add_edge(\"trim\", END)\n",
    "\n",
    "# 编译图\n",
    "compress_workflow = compress_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd584237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 演示'总结压缩'技术 ###\n",
      "\n",
      "--- COMPRESSING LONG TEXT ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 文本压缩: 682 tokens → 225 tokens (节省 457 tokens)\n",
      "📝 压缩摘要:\n",
      "2023年，GPT-4与Claude 2.1分别实现32K和200K tokens上下文窗口。2024年，DeepSeek发布DeepSeek-R1，支持128K tokens，并引入“上下文压缩”技术，可将长文档压缩至原长的20%-30%，保留95%以上核心信息。其技术创新包括分层总结架构、语义密度优化及自适应压缩率。测试中，10,000字论文压缩至1,500字，任务质量仅下降2%，Token使...\n",
      "{'compress': {'compressed_text': '2023年，GPT-4与Claude 2.1分别实现32K和200K tokens上下文窗口。2024年，DeepSeek发布DeepSeek-R1，支持128K tokens，并引入“上下文压缩”技术，可将长文档压缩至原长的20%-30%，保留95%以上核心信息。其技术创新包括分层总结架构、语义密度优化及自适应压缩率。测试中，10,000字论文压缩至1,500字，任务质量仅下降2%，Token使用量减少82%。该技术适用于法律、金融、学术及客服等领域，未来将探索动态上下文压缩以提升长期记忆管理能力。', 'token_savings': 457, 'messages': [HumanMessage(content='基于以下摘要回答问题:\\n2023年，GPT-4与Claude 2.1分别实现32K和200K tokens上下文窗口。2024年，DeepSeek发布DeepSeek-R1，支持128K tokens，并引入“上下文压缩”技术，可将长文档压缩至原长的20%-30%，保留95%以上核心信息。其技术创新包括分层总结架构、语义密度优化及自适应压缩率。测试中，10,000字论文压缩至1,500字，任务质量仅下降2%，Token使用量减少82%。该技术适用于法律、金融、学术及客服等领域，未来将探索动态上下文压缩以提升长期记忆管理能力。\\n\\n问题: DeepSeek-R1在文本压缩方面有哪些技术创新？压缩效果如何？', additional_kwargs={}, response_metadata={})]}}\n",
      "---\n",
      "\n",
      "--- ANSWERING WITH COMPRESSED TEXT ---\n",
      "💡 生成的回答: DeepSeek-R1在文本压缩方面的主要技术创新包括：\n",
      "\n",
      "1. **分层总结架构**：通过多层级的摘要机制，对文本内容进行结构化提炼，确保关键信息不丢失。\n",
      "2. **语义密度优化**：提升压缩过程中对语义信息的保留能力，使压缩后的文本仍能传达原意。\n",
      "3. **自适应压缩率**：根据文本内容动态调整压缩比例，实现更灵活、高效的压缩效果。\n",
      "\n",
      "**压缩效果**方面，DeepSeek-R1能够将长文档压...\n",
      "{'answer': {'messages': [AIMessage(content='DeepSeek-R1在文本压缩方面的主要技术创新包括：\\n\\n1. **分层总结架构**：通过多层级的摘要机制，对文本内容进行结构化提炼，确保关键信息不丢失。\\n2. **语义密度优化**：提升压缩过程中对语义信息的保留能力，使压缩后的文本仍能传达原意。\\n3. **自适应压缩率**：根据文本内容动态调整压缩比例，实现更灵活、高效的压缩效果。\\n\\n**压缩效果**方面，DeepSeek-R1能够将长文档压缩至原长度的20%-30%，同时保留95%以上的核心信息。例如，在测试中，10,000字的论文被压缩至1,500字，任务质量仅下降2%，而Token使用量减少了82%。这表明其在保持信息完整性的同时显著降低了计算和存储成本。', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'qwen3-30b-a3b'}, id='run--076fdcd4-d5c4-45a8-b187-706148a6a2fc-0', usage_metadata={'input_tokens': 222, 'output_tokens': 185, 'total_tokens': 407, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 演示总结压缩 ---\n",
    "print(\"\\n### 演示'总结压缩'技术 ###\")\n",
    "question = \"DeepSeek-R1在文本压缩方面有哪些技术创新？压缩效果如何？\"\n",
    "\n",
    "# 初始状态\n",
    "initial_state = CompressState(\n",
    "    messages=[SystemMessage(content=\"你是一个AI技术分析师\"), HumanMessage(content=question)],\n",
    "    original_text=long_article,\n",
    "    compressed_text=\"\",\n",
    "    token_savings=0\n",
    ")\n",
    "\n",
    "# 执行工作流\n",
    "for step in compress_workflow.stream(initial_state):\n",
    "    if \"__end__\" not in step:\n",
    "        print(step)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77dc2c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 演示'裁剪压缩'技术 ###\n",
      "\n",
      "--- TRIMMING CONTEXT ---\n",
      "✂️ 裁剪历史: 10条 → 4条消息\n",
      "📉 Token节省: 92 (原始: 150 -> 裁剪后: 58)\n",
      "\n",
      "裁剪前消息:\n",
      "🤖 你是一个专业的旅行助手\n",
      "👤 我想计划一次去日本的旅行\n",
      "👤 时间大概是明年3月下旬，10天左右\n",
      "👤 我对京都的文化景点特别感兴趣\n",
      "👤 另外也想体验一下东京的现代化都市\n",
      "👤 预算方面希望控制在2万元以内\n",
      "👤 请帮我规划一个行程\n",
      "👤 对了，我还想体验一次温泉旅馆\n",
      "👤 最好是那种传统的日式旅馆\n",
      "👤 现在请给我具体的行程建议\n",
      "\n",
      "裁剪后消息:\n",
      "🤖 你是一个专业的旅行助手\n",
      "👤 对了，我还想体验一次温泉旅馆\n",
      "👤 最好是那种传统的日式旅馆\n",
      "👤 现在请给我具体的行程建议\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 演示对话历史裁剪 ---\n",
    "print(\"\\n### 演示'裁剪压缩'技术 ###\")\n",
    "\n",
    "# 创建一个长对话历史\n",
    "long_chat_history = [\n",
    "    SystemMessage(content=\"你是一个专业的旅行助手\"),\n",
    "    HumanMessage(content=\"我想计划一次去日本的旅行\"),\n",
    "    HumanMessage(content=\"时间大概是明年3月下旬，10天左右\"),\n",
    "    HumanMessage(content=\"我对京都的文化景点特别感兴趣\"),\n",
    "    HumanMessage(content=\"另外也想体验一下东京的现代化都市\"),\n",
    "    HumanMessage(content=\"预算方面希望控制在2万元以内\"),\n",
    "    HumanMessage(content=\"请帮我规划一个行程\"),\n",
    "    HumanMessage(content=\"对了，我还想体验一次温泉旅馆\"),\n",
    "    HumanMessage(content=\"最好是那种传统的日式旅馆\"),\n",
    "    HumanMessage(content=\"现在请给我具体的行程建议\")\n",
    "]\n",
    "\n",
    "# 初始状态（无文本压缩）\n",
    "trim_demo_state = CompressState(\n",
    "    messages=long_chat_history,\n",
    "    original_text=\"\",\n",
    "    compressed_text=\"\",\n",
    "    token_savings=0\n",
    ")\n",
    "\n",
    "# 执行裁剪\n",
    "trimmed_state = trim_context(trim_demo_state)\n",
    "\n",
    "# 显示裁剪效果\n",
    "print(\"\\n裁剪前消息:\")\n",
    "for i, msg in enumerate(long_chat_history):\n",
    "    prefix = \"🤖\" if isinstance(msg, SystemMessage) else \"👤\"\n",
    "    print(f\"{prefix} {msg.content[:50]}{'...' if len(msg.content) > 50 else ''}\")\n",
    "\n",
    "print(\"\\n裁剪后消息:\")\n",
    "for i, msg in enumerate(trimmed_state['messages']):\n",
    "    prefix = \"🤖\" if isinstance(msg, SystemMessage) else \"👤\"\n",
    "    print(f\"{prefix} {msg.content[:50]}{'...' if len(msg.content) > 50 else ''}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa94b02",
   "metadata": {},
   "source": [
    "## **策略四：隔离 (Isolate) - \"分而治之\"的架构智慧**\n",
    "\n",
    "**核心思想：** 将复杂任务分解为多个子任务，由专门的智能体在隔离环境中处理，避免上下文污染。\n",
    "\n",
    "**实现多智能体架构：** 创建专家智能体团队（分析师+文案）\n",
    "\n",
    "**场景演示：** 用户上传销售数据CSV，要求分析销售冠军并撰写营销文案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca6ab70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 准备数据 ---\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# 创建示例销售数据CSV\n",
    "sales_data = \"\"\"\n",
    "日期,产品,销售额,销售量\n",
    "2024-01-01,机械键盘,12800,32\n",
    "2024-01-01,游戏鼠标,9800,49\n",
    "2024-01-02,机械键盘,14500,36\n",
    "2024-01-02,游戏鼠标,10200,51\n",
    "2024-01-03,机械键盘,16200,40\n",
    "2024-01-03,游戏鼠标,10800,54\n",
    "2024-01-04,机械键盘,13800,34\n",
    "2024-01-04,游戏鼠标,11200,56\n",
    "2024-01-05,机械键盘,17500,42\n",
    "2024-01-05,游戏鼠标,11800,59\n",
    "\"\"\"\n",
    "\n",
    "# 保存为CSV文件\n",
    "with open(\"sales_data.csv\", \"w\") as f:\n",
    "    f.write(sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bcf219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. 定义状态 ---\n",
    "# 定义多智能体协作的状态\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    task: str\n",
    "    analysis_result: str\n",
    "    final_output: str\n",
    "    next_agent: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22cb0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 定义工具 ---\n",
    "@tool\n",
    "def analyze_sales_data(question: str) -> str:\n",
    "    \"\"\"\n",
    "    分析销售数据CSV文件，找出销售额最高的产品及其销售总额。\n",
    "    参数:\n",
    "        question (str): 用户的原始问题，用于记录分析背景。\n",
    "    返回:\n",
    "        str: 一个逗号分隔的字符串，包含产品名称和总销售额，例如 \"产品A,150000\"。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- TOOL: ANALYZE SALES DATA ---\")\n",
    "    print(f\"📝 分析任务: {question}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\"sales_data.csv\")\n",
    "        product_sales = df.groupby(\"产品\")[\"销售额\"].sum()\n",
    "        top_product = product_sales.idxmax()\n",
    "        top_sales = product_sales.max()\n",
    "        result = f\"{top_product},{top_sales}\"\n",
    "        print(f\"🏆 分析结果: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"分析失败: {e}\"\n",
    "\n",
    "@tool\n",
    "def write_marketing_copy(product: str, key_points: str) -> str:\n",
    "    \"\"\"\n",
    "    为指定产品撰写营销文案。\n",
    "    参数:\n",
    "        product (str): 需要撰写文案的产品名称。\n",
    "        key_points (str): 文案需要围绕的核心卖点。\n",
    "    返回:\n",
    "        str: 生成的营销文案。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- TOOL: WRITE MARKETING COPY ---\")\n",
    "    print(f\"📝 撰写文案: {product} - {key_points[:50]}...\")\n",
    "    \n",
    "    writer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"你是一个专业营销文案。请基于以下产品信息撰写一篇不超过150字的吸引人的营销文案:\\n\"\n",
    "        \"产品名称: {product}\\n\"\n",
    "        \"核心卖点: {key_points}\\n\"\n",
    "        \"文案:\"\n",
    "    )\n",
    "    writer_chain = writer_prompt | model | StrOutputParser()\n",
    "    \n",
    "    return writer_chain.invoke({\"product\": product, \"key_points\": key_points})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd1b3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 创建智能体 ---\n",
    "\n",
    "# Helper function to create a specialist agent\n",
    "def create_agent(system_prompt: str, tools: list):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ])\n",
    "    agent = prompt | model.bind_tools(tools)\n",
    "    return agent\n",
    "\n",
    "# 分析师智能体\n",
    "analyst_agent = create_agent(\n",
    "    \"你是一名专业的数据分析师。你的任务是分析给定的数据并返回关键结果。请使用`analyze_sales_data`工具来完成任务。\",\n",
    "    [analyze_sales_data]\n",
    ")\n",
    "\n",
    "# 文案智能体\n",
    "writer_agent = create_agent(\n",
    "    \"你是一名专业的营销文案。你的任务是根据分析结果，为产品撰写引人注目的营销文案。请使用`write_marketing_copy`工具来完成任务。\",\n",
    "    [write_marketing_copy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5334c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. 定义智能体节点 ---\n",
    "\n",
    "def analyst_node(state: AgentState):\n",
    "    print(\"\\n--- CALLING ANALYST AGENT ---\")\n",
    "    result = analyst_agent.invoke({\"messages\": [HumanMessage(content=state['task'])]})\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "def writer_node(state: AgentState):\n",
    "    print(\"\\n--- CALLING WRITER AGENT ---\")\n",
    "    # 从state中提取分析结果，并作为输入传递给文案智能体\n",
    "    product, sales = state['analysis_result'].split(',')\n",
    "    prompt = f\"分析结果：销售冠军是‘{product}’，总销售额为 {sales} 元。请为此产品撰写营销文案。\"\n",
    "    result = writer_agent.invoke({\"messages\": [HumanMessage(content=prompt)]})\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "# 定义工具执行节点\n",
    "tool_node = ToolNode([analyze_sales_data, write_marketing_copy])\n",
    "\n",
    "def execute_tools(state: AgentState):\n",
    "    print(\"\\n--- EXECUTING TOOLS ---\")\n",
    "    last_message = state['messages'][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    \n",
    "    # 执行工具\n",
    "    tool_result = tool_node.invoke([last_message])\n",
    "    \n",
    "    # 根据工具更新状态\n",
    "    if tool_call['name'] == 'analyze_sales_data':\n",
    "        return {\"messages\": tool_result, \"analysis_result\": tool_result[0].content}\n",
    "    elif tool_call['name'] == 'write_marketing_copy':\n",
    "        return {\"messages\": tool_result, \"final_output\": tool_result[0].content}\n",
    "    \n",
    "    return {\"messages\": tool_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea160988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 构建图 (Supervisor模式) ---\n",
    "\n",
    "def supervisor_router(state: AgentState):\n",
    "    \"\"\"路由：决定下一个应该由哪个智能体来处理\"\"\"\n",
    "    print(\"\\n--- SUPERVISOR ---\")\n",
    "\n",
    "    # 如果分析结果还未产生，则分配给分析师\n",
    "    if not state.get(\"analysis_result\"):\n",
    "        print(\"📋 任务分配: 分析师 (Analyst)\")\n",
    "        return \"analyst\"\n",
    "        \n",
    "    # 如果分析已完成但文案还未撰写，则分配给文案\n",
    "    if state.get(\"analysis_result\") and not state.get(\"final_output\"):\n",
    "        print(\"📋 任务分配: 文案撰写 (Writer)\")\n",
    "        return \"writer\"\n",
    "        \n",
    "    # 如果一切都完成了\n",
    "    print(\"✅ 所有任务完成\")\n",
    "    return END\n",
    "\n",
    "# 构建图\n",
    "isolate_graph = StateGraph(AgentState)\n",
    "\n",
    "isolate_graph.add_node(\"analyst\", analyst_node)\n",
    "isolate_graph.add_node(\"writer\", writer_node)\n",
    "isolate_graph.add_node(\"execute_tools\", execute_tools)\n",
    "\n",
    "# 设置入口点\n",
    "isolate_graph.set_entry_point(\"analyst\")\n",
    "\n",
    "# 定义图的边\n",
    "isolate_graph.add_edge(\"analyst\", \"execute_tools\")\n",
    "isolate_graph.add_edge(\"writer\", \"execute_tools\")\n",
    "isolate_graph.add_conditional_edges(\n",
    "    \"execute_tools\",\n",
    "    supervisor_router,\n",
    "    {\"analyst\": \"analyst\", \"writer\": \"writer\", END: END}\n",
    ")\n",
    "\n",
    "# 编译工作流\n",
    "isolate_workflow = isolate_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e0bce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 演示多智能体协作 (Supervisor模式) ###\n",
      "\n",
      "--- CALLING ANALYST AGENT ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [analyst] 步骤完成 ---\n",
      "\n",
      "--- EXECUTING TOOLS ---\n",
      "\n",
      "--- TOOL: ANALYZE SALES DATA ---\n",
      "📝 分析任务: 分析销售数据找出销售额最高的产品，然后为该产品撰写一篇吸引人的营销文案。\n",
      "🏆 分析结果: 机械键盘,74800\n",
      "\n",
      "--- SUPERVISOR ---\n",
      "📋 任务分配: 文案撰写 (Writer)\n",
      "--- [execute_tools] 步骤完成 ---\n",
      "\n",
      "--- CALLING WRITER AGENT ---\n",
      "--- [writer] 步骤完成 ---\n",
      "\n",
      "--- EXECUTING TOOLS ---\n",
      "\n",
      "--- TOOL: WRITE MARKETING COPY ---\n",
      "📝 撰写文案: 机械键盘 - 销售冠军，总销售额为 74800 元...\n",
      "\n",
      "--- SUPERVISOR ---\n",
      "✅ 所有任务完成\n",
      "--- [execute_tools] 步骤完成 ---\n",
      "\n",
      "🎉 最终文案:\n",
      "销量冠军，口碑之选！这款机械键盘凭借卓越性能与出色手感，热销至今，总销售额突破74800元。无论是游戏还是办公，都能带来畅快体验，是每一位键盘控的不二之选！\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 执行多智能体协作 ---\n",
    "print(\"\\n### 演示多智能体协作 (Supervisor模式) ###\")\n",
    "task = (\n",
    "    \"分析销售数据找出销售额最高的产品，\"\n",
    "    \"然后为该产品撰写一篇吸引人的营销文案。\"\n",
    ")\n",
    "initial_state = AgentState(\n",
    "    messages=[],\n",
    "    task=task,\n",
    "    analysis_result=\"\",\n",
    "    final_output=\"\",\n",
    "    next_agent=\"analyst\"\n",
    ")\n",
    "\n",
    "# 执行工作流\n",
    "for step in isolate_workflow.stream(initial_state, {\"recursion_limit\": 10}):\n",
    "    node = list(step.keys())[0]\n",
    "    state = step[node]\n",
    "    print(f\"--- [{node}] 步骤完成 ---\")\n",
    "    if \"final_output\" in state and state[\"final_output\"]:\n",
    "        print(f\"\\n🎉 最终文案:\\n{state['final_output']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
